{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Satellite-based monitoring of dry and wet conditions using Standardized Precipitation Index (SPI) The Standardized Precipitation Index (SPI) analysis is following the training conducted in 28 Jan 2020 by NASA ARSET on Application of GPM IMERG Reanalysis for Assessing Extreme Dry and Wet Periods . https://appliedsciences.nasa.gov/join-mission/training/english/arset-applications-gpm-imerg-reanalysis-assessing-extreme-dry-and-wet The training session from NASA ARSET provided information on how to access and download IMERG data, and use it to calculate SPI on defined time scales. Many participant experience several problems and try to raise some question to the developer of climate-indices python package in their Github page and some also ask in StackExchange . I also experience several problem during the training and try to documented the solution by modified some step on their guideline. In this site, I would like to re-share on how to calculate SPI using NASA ARSET approach and provide alternative way using different data and format. This how-to guideline will use latest version of Climate Indices in Python software. While NASA ARSET training still used the official release version from U.S. Drought Portal On How-to? section, you will find step-by-step guideline to calculate SPI, and can try different (data source and format) approach below: SPI based on IMERG data in netCDF format (following NASA ARSET training but adjusted in some step) SPI based on CHIRPS data GeoTIFF format SPI based on CHIRPS data in netCDF format Notes This step-by-step guide was tested using Macbook Pro, 2.9 GHz 6-Core Intel Core i9, 32 GB 2400 MHz DDR4, running on macOS Catalina 10.15.7 And a Windows Server 2019 running in Parallels Desktop for Mac , with Windows Subsystem for Linux (WSL) installed. Info I will use a standard WSL 2 for Windows 10 as most of Windows user are using Windows 10 for their works. Contact For further information about this guideline, please contact: Benny Istanto Earth Observation and Climate Analyst Vulnerability Analysis and Mapping Unit UN World Food Programme Jakarta, Indonesia E. benny.istanto@wfp.org","title":"About"},{"location":"#satellite-based-monitoring-of-dry-and-wet-conditions-using-standardized-precipitation-index-spi","text":"The Standardized Precipitation Index (SPI) analysis is following the training conducted in 28 Jan 2020 by NASA ARSET on Application of GPM IMERG Reanalysis for Assessing Extreme Dry and Wet Periods . https://appliedsciences.nasa.gov/join-mission/training/english/arset-applications-gpm-imerg-reanalysis-assessing-extreme-dry-and-wet The training session from NASA ARSET provided information on how to access and download IMERG data, and use it to calculate SPI on defined time scales. Many participant experience several problems and try to raise some question to the developer of climate-indices python package in their Github page and some also ask in StackExchange . I also experience several problem during the training and try to documented the solution by modified some step on their guideline. In this site, I would like to re-share on how to calculate SPI using NASA ARSET approach and provide alternative way using different data and format. This how-to guideline will use latest version of Climate Indices in Python software. While NASA ARSET training still used the official release version from U.S. Drought Portal On How-to? section, you will find step-by-step guideline to calculate SPI, and can try different (data source and format) approach below: SPI based on IMERG data in netCDF format (following NASA ARSET training but adjusted in some step) SPI based on CHIRPS data GeoTIFF format SPI based on CHIRPS data in netCDF format","title":"Satellite-based monitoring of dry and wet conditions using Standardized Precipitation Index (SPI)"},{"location":"#notes","text":"This step-by-step guide was tested using Macbook Pro, 2.9 GHz 6-Core Intel Core i9, 32 GB 2400 MHz DDR4, running on macOS Catalina 10.15.7 And a Windows Server 2019 running in Parallels Desktop for Mac , with Windows Subsystem for Linux (WSL) installed. Info I will use a standard WSL 2 for Windows 10 as most of Windows user are using Windows 10 for their works.","title":"Notes"},{"location":"#contact","text":"For further information about this guideline, please contact: Benny Istanto Earth Observation and Climate Analyst Vulnerability Analysis and Mapping Unit UN World Food Programme Jakarta, Indonesia E. benny.istanto@wfp.org","title":"Contact"},{"location":"background/","text":"Background on the SPI The Standardized Precipitation Index (SPI) is a drought index first developed by T. B. McKee, N.J. Doesken, and J. Kleist and in 1993 ( McKee et al. 1993 ). The SPI is used for estimating wet or dry condition based on precipitation variable. This wet or dry condition can be monitored by the SPI on a variety of time scales from subseasonal to interannual scales. The SPI is expressed as standard deviations that the observed precipitation would deviate from the long-term mean, for a normal distribution and fitted probability distribution for the actual precipitation record. Since precipitation is not normally distributed, a transformation is first applied, followed by fitting to a normal distribution. Calculation The SPI calculation is based on the long-term precipitation record for a particular location and long-term period (longer than 30 years is desirable). The calculation method is comprised of a transformation of one frequency distribution (e.g., gamma) to another frequency distribution (normal, or Gaussian). The first step to calculate SPI is to adequately choose a particular probability distribution (e.g., gamma distribution, incomplete beta distribution (McKee et al. ( 1993 , 1995 )), and Pearson III distribution (Guttman ( 1998 , 1999 ))) that reliably fits the long-term precipitation time series and conduct fitting to that distribution. Gamma distribution has been widely used, as the gamma distribution has been understood as the reliable fit to the precipitation distribution. The fitting can be achieved through the maximum likelihood estimation of the gamma distribution parameters. The percentile value from this probability distribution is then transformed to the corresponding value in the new probability distribution. As a result, the probability that the rainfall is less than or equal to any rainfall amount will be the same as the probability that the new variate is less than or equal to the corresponding value of that rainfall amount. The normal distribution is usually used for this another transformation so that the mean and standard deviation of the SPI for a certain station and long-term period is zero and one, respectively ( Edwards and McKee 1997 ). Positive SPI values indicate wet condition greater than median precipitation, whereas negative values the dry condition less than median precipitation. More detailed description of the steps required to calculate the SPI is provided in Lloyd-Hughes and Saunders (2002) . Interpretation Since the SPI values are obtained from the standard normal distribution, the unit of the SPI can be \u201cstandard deviations\u201d. The following table summarizes the cumulative probabilities for various SPI values and possible interpretation of wet (or dry) conditions using the resulting SPI values. SPI Cumulative Probability Interpretation -3.0 0.0014 Extremely dry -2.5 0.0062 Extremely dry -2.0 0.0228 Extremely dry (SPI < -2.0) -1.5 0.0668 Severly dry (-2.0 < SPI < -1.5) -1.0 0.1587 Moderately dry (-1.5 < SPI < -1.0) -0.5 0.3085 Near normal 0.0 0.5000 Near normal 0.5 0.6915 Near normal 1.0 0.8413 Moderately wet (1.0 < SPI < 1.5) 1.5 0.9332 Severly wet (1.5 < SPI < 2.0) 2.0 0.9772 Extremely wet (2.0 < SPI <) 2.5 0.9938 Extremely wet 3.0 0.9986 Extremely wet The SPI maps can be interpreted at various time scales. This in turn indicates that the SPI is useful in both short-term and long-term applications. These time scales reflect the impact of drought on the availability of the different water resources. For instance, soil moisture conditions respond to precipitation anomalies on a relatively short scale. Groundwater, streamflow, and reservoir storage reflect the longer-term precipitation anomalies. For these reasons, SPI was originally calculated for 3\u2013, 6\u2013,12\u2013, 24\u2013, and 48\u2013month time scales ( McKee et al. (1993) ). A separate SPI value can be calculated for a selection of time scales, covering the last months (e.g., 3, 6, 12, 24, and 48 months), and ending on the last day of the latest month. Source: https://gmao.gsfc.nasa.gov/research/subseasonal/atlas/SPI-html/SPI-description.html Strengths and Limitations Used for estimating meteorological conditions based on precipitation alone. Wet or dry conditions can be monitored on a variety of time scales from sub seasonal to interannual Can be compared across regions with markedly difference climates Does not consider the intensity of precipitation and its potential impacts on runoff, streamflow, and water availability Expressed as the number of standard deviations from the long term mean, for a normally distributed random variable, and fitted probability distribution for the actual precipitation record SPI values < 1 indicate a condition of drought, the more negative the value the more severe the drought condition. SPI values > +1 indicate wetter conditions compared to a climatology Example Expressed as the number of standard deviations from the long-term mean, for a normally distributed random variable, and fitted probability distribution for the actual precipitation record SPI values < -1 indicate a condition of drought, the more negative the value the more severe the drought condition. SPI values > +1 indicate wetter conditions compared to a climatology https://drought.unl.edu/droughtmonitoring/SPI.aspx SPI 1-month Similar to a map displaying the percent of normal precipitation for a month. Reflects relatively short term conditions. Its application can be related closely with short term soil moisture and crop stress. 3 month Provides a comparison of the precipitation over a specific 3 month period with the precipitation totals from the same 3 month period for all the years included in the historical record. Reflects short and medium term moisture conditions and provides a seasonal estimation of precipitation. 6 month Compares the precipitation for that period with the same 6 month period over the historical record. A 6 month SPI can be very effective in showing the precipitation over distinct seasons and may be associated with anomalous streamflow and reservoir levels. 9 month Provides an indication of precipitation patterns over a medium time scale. SPI values below 1.5 for these time scales are usually a good indication that significant impacts are occurring in agriculture and may be showing up in other sectors as well. 12 month Reflects long term precipitation patterns. Longer SPIs tend toward zero unless a specific trend is taking place. SPIs of these time scales are probably tied to streamflow, reservoir levels, and even groundwater levels at the longer time scales. In some locations of the country, the 12 month SPI is most closely related with the Palmer Index, and the two indices should reflect similar conditions. SPI labels and their relationship to the normal curve. The intensity implied by each label corresponds to the degree of removal from mean conditions (i.e., SPI=0). The percentages printed within the regions bounded by the dashed lines indicate the probability for SPI values to fall within that region only. (Source: J. Keyantash )","title":"Background"},{"location":"background/#background-on-the-spi","text":"The Standardized Precipitation Index (SPI) is a drought index first developed by T. B. McKee, N.J. Doesken, and J. Kleist and in 1993 ( McKee et al. 1993 ). The SPI is used for estimating wet or dry condition based on precipitation variable. This wet or dry condition can be monitored by the SPI on a variety of time scales from subseasonal to interannual scales. The SPI is expressed as standard deviations that the observed precipitation would deviate from the long-term mean, for a normal distribution and fitted probability distribution for the actual precipitation record. Since precipitation is not normally distributed, a transformation is first applied, followed by fitting to a normal distribution.","title":"Background on the SPI"},{"location":"background/#calculation","text":"The SPI calculation is based on the long-term precipitation record for a particular location and long-term period (longer than 30 years is desirable). The calculation method is comprised of a transformation of one frequency distribution (e.g., gamma) to another frequency distribution (normal, or Gaussian). The first step to calculate SPI is to adequately choose a particular probability distribution (e.g., gamma distribution, incomplete beta distribution (McKee et al. ( 1993 , 1995 )), and Pearson III distribution (Guttman ( 1998 , 1999 ))) that reliably fits the long-term precipitation time series and conduct fitting to that distribution. Gamma distribution has been widely used, as the gamma distribution has been understood as the reliable fit to the precipitation distribution. The fitting can be achieved through the maximum likelihood estimation of the gamma distribution parameters. The percentile value from this probability distribution is then transformed to the corresponding value in the new probability distribution. As a result, the probability that the rainfall is less than or equal to any rainfall amount will be the same as the probability that the new variate is less than or equal to the corresponding value of that rainfall amount. The normal distribution is usually used for this another transformation so that the mean and standard deviation of the SPI for a certain station and long-term period is zero and one, respectively ( Edwards and McKee 1997 ). Positive SPI values indicate wet condition greater than median precipitation, whereas negative values the dry condition less than median precipitation. More detailed description of the steps required to calculate the SPI is provided in Lloyd-Hughes and Saunders (2002) .","title":"Calculation"},{"location":"background/#interpretation","text":"Since the SPI values are obtained from the standard normal distribution, the unit of the SPI can be \u201cstandard deviations\u201d. The following table summarizes the cumulative probabilities for various SPI values and possible interpretation of wet (or dry) conditions using the resulting SPI values. SPI Cumulative Probability Interpretation -3.0 0.0014 Extremely dry -2.5 0.0062 Extremely dry -2.0 0.0228 Extremely dry (SPI < -2.0) -1.5 0.0668 Severly dry (-2.0 < SPI < -1.5) -1.0 0.1587 Moderately dry (-1.5 < SPI < -1.0) -0.5 0.3085 Near normal 0.0 0.5000 Near normal 0.5 0.6915 Near normal 1.0 0.8413 Moderately wet (1.0 < SPI < 1.5) 1.5 0.9332 Severly wet (1.5 < SPI < 2.0) 2.0 0.9772 Extremely wet (2.0 < SPI <) 2.5 0.9938 Extremely wet 3.0 0.9986 Extremely wet The SPI maps can be interpreted at various time scales. This in turn indicates that the SPI is useful in both short-term and long-term applications. These time scales reflect the impact of drought on the availability of the different water resources. For instance, soil moisture conditions respond to precipitation anomalies on a relatively short scale. Groundwater, streamflow, and reservoir storage reflect the longer-term precipitation anomalies. For these reasons, SPI was originally calculated for 3\u2013, 6\u2013,12\u2013, 24\u2013, and 48\u2013month time scales ( McKee et al. (1993) ). A separate SPI value can be calculated for a selection of time scales, covering the last months (e.g., 3, 6, 12, 24, and 48 months), and ending on the last day of the latest month. Source: https://gmao.gsfc.nasa.gov/research/subseasonal/atlas/SPI-html/SPI-description.html","title":"Interpretation"},{"location":"background/#strengths-and-limitations","text":"Used for estimating meteorological conditions based on precipitation alone. Wet or dry conditions can be monitored on a variety of time scales from sub seasonal to interannual Can be compared across regions with markedly difference climates Does not consider the intensity of precipitation and its potential impacts on runoff, streamflow, and water availability Expressed as the number of standard deviations from the long term mean, for a normally distributed random variable, and fitted probability distribution for the actual precipitation record SPI values < 1 indicate a condition of drought, the more negative the value the more severe the drought condition. SPI values > +1 indicate wetter conditions compared to a climatology","title":"Strengths and Limitations"},{"location":"background/#example","text":"Expressed as the number of standard deviations from the long-term mean, for a normally distributed random variable, and fitted probability distribution for the actual precipitation record SPI values < -1 indicate a condition of drought, the more negative the value the more severe the drought condition. SPI values > +1 indicate wetter conditions compared to a climatology https://drought.unl.edu/droughtmonitoring/SPI.aspx SPI 1-month Similar to a map displaying the percent of normal precipitation for a month. Reflects relatively short term conditions. Its application can be related closely with short term soil moisture and crop stress. 3 month Provides a comparison of the precipitation over a specific 3 month period with the precipitation totals from the same 3 month period for all the years included in the historical record. Reflects short and medium term moisture conditions and provides a seasonal estimation of precipitation. 6 month Compares the precipitation for that period with the same 6 month period over the historical record. A 6 month SPI can be very effective in showing the precipitation over distinct seasons and may be associated with anomalous streamflow and reservoir levels. 9 month Provides an indication of precipitation patterns over a medium time scale. SPI values below 1.5 for these time scales are usually a good indication that significant impacts are occurring in agriculture and may be showing up in other sectors as well. 12 month Reflects long term precipitation patterns. Longer SPIs tend toward zero unless a specific trend is taking place. SPIs of these time scales are probably tied to streamflow, reservoir levels, and even groundwater levels at the longer time scales. In some locations of the country, the 12 month SPI is most closely related with the Palmer Index, and the two indices should reflect similar conditions. SPI labels and their relationship to the normal curve. The intensity implied by each label corresponds to the degree of removal from mean conditions (i.e., SPI=0). The percentages printed within the regions bounded by the dashed lines indicate the probability for SPI values to fall within that region only. (Source: J. Keyantash )","title":"Example"},{"location":"chirpsnc/","text":"3.3. CHIRPS monthly in netCDF format This section will explain on how to download CHIRPS monthly data in netCDF format and prepare it as input for SPI calculation. Make sure you still inside conda gis environment Download CHIRPS data Navigate to Downloads/CHIRPS/netCDF/Original folder in the working directory. Download using wget all CHIRPS dekad data in netCDF format from Jan 1981 to Apr 2021 (this is lot of data +- 6.4GB, please make sure you have bandwidth and unlimited data package). Paste and Enter below script in your Terminal. wget -c https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/netcdf/chirps-v2.0.monthly.nc As an alternative, you can download via ftp client like FileZilla . Below is exampe using ftp client Transmit for Mac . Clip data using a bounding box based on area of interest I am providing example on how to use CDO and NCO to do some data extraction process, you can choose which one is suits you. In my opinion, NCO is faster than CDO, and NCO produce smaller size of output. Crop your area of interest using bounding box. Example: Java bounding box with format lon1 , lon2 , lat1 , lat2 is 105.05 , 116.25 , -8.8 , -5.05 Paste and Enter below code in your Terminal. CDO script: cdo sellonlatbox,105.05,116.25,-8.8,-5.05 chirps-v2.0.monthly.nc ../Clipped/java_chirps-v2.0.monthly.nc NCO script ncks -d latitude,-8.8,-5.05 -d longitude,105.05,116.25 chirps-v2.0.monthly.nc -O ../Clipped/java_chirps-v2.0.monthly.nc Let's read header contents of above result. Type and execute below code: ncdump -h java_chirps-v2.0.monthly.nc You will get information (dimension, variables and global attribute) about the data. Edit variable and attribute As explain in Input specification , we can say from above picture there are few variable and attribute that need editing. Let's do it one-by-one. Navigate to Downloads/CHIRPS/netCDF/Clipped folder in the working directory. Variable Edit variable name for longitude to lon , and latitude to lat Paste and Enter line-by-line CDO script below in Terminal. cdo chname,longitude,lon java_chirps-v2.0.monthly.nc java_chirps-v2.0.monthly_1.nc cdo chname,latitude,lat java_chirps-v2.0.monthly_1.nc java_chirps-v2.0.monthly_2.nc Alternative, using NCO script is below ncrename -d longitude,lon -d latitude,lat -v longitude,lon -v latitude,lat java_chirps-v2.0.monthly.nc -O java_chirps-v2.0.monthly_2.nc Let's read header contents of above result. Type and execute below code: ncdump -h java_chirps-v2.0.monthly_2.nc Result from CDO Result from NCO Attribute Edit precipitation unit from mm/month to mm Paste and Enter line-by-line CDO script below in Terminal. cdo -setattribute,precip@units = \"mm\" java_chirps-v2.0.monthly_2.nc java_chirps-v2.0.monthly_3.nc Alternative, using NCO script is below ncatted -a units,precip,modify,c, 'mm' java_chirps-v2.0.monthly_2.nc java_chirps-v2.0.monthly_3.nc Let's read header contents of above result. Type and execute below code: ncdump -h java_chirps-v2.0.monthly_3.nc Result from CDO Result from NCO And the units already in mm Once this has completed, the dataset can be used as input to this package for computing SPI. From above picture, some of the precipitation attribute are still wrong: DimensionNames and Units . I can leave it as is, SPI code will only read units and variables precip(time,lat,lon) As the input data preparation is completed, move the file java_chirps-v2.0.monthly_3.nc to main folder Input_nc and rename into java_cli_imerg_1months_1981_2020.nc mv java_chirps-v2.0.monthly_3.nc ../../../../Input_nc/java_cli_imerg_1months_2000_2020.nc Make sure the file java_cli_imerg_1months_1981_2020.nc is available at Input_nc folder","title":"3.3 CHIRPS monthly netCDF"},{"location":"chirpsnc/#33-chirps-monthly-in-netcdf-format","text":"This section will explain on how to download CHIRPS monthly data in netCDF format and prepare it as input for SPI calculation. Make sure you still inside conda gis environment","title":"3.3. CHIRPS monthly in netCDF format"},{"location":"chirpsnc/#download-chirps-data","text":"Navigate to Downloads/CHIRPS/netCDF/Original folder in the working directory. Download using wget all CHIRPS dekad data in netCDF format from Jan 1981 to Apr 2021 (this is lot of data +- 6.4GB, please make sure you have bandwidth and unlimited data package). Paste and Enter below script in your Terminal. wget -c https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/netcdf/chirps-v2.0.monthly.nc As an alternative, you can download via ftp client like FileZilla . Below is exampe using ftp client Transmit for Mac .","title":"Download CHIRPS data"},{"location":"chirpsnc/#clip-data-using-a-bounding-box-based-on-area-of-interest","text":"I am providing example on how to use CDO and NCO to do some data extraction process, you can choose which one is suits you. In my opinion, NCO is faster than CDO, and NCO produce smaller size of output. Crop your area of interest using bounding box. Example: Java bounding box with format lon1 , lon2 , lat1 , lat2 is 105.05 , 116.25 , -8.8 , -5.05 Paste and Enter below code in your Terminal. CDO script: cdo sellonlatbox,105.05,116.25,-8.8,-5.05 chirps-v2.0.monthly.nc ../Clipped/java_chirps-v2.0.monthly.nc NCO script ncks -d latitude,-8.8,-5.05 -d longitude,105.05,116.25 chirps-v2.0.monthly.nc -O ../Clipped/java_chirps-v2.0.monthly.nc Let's read header contents of above result. Type and execute below code: ncdump -h java_chirps-v2.0.monthly.nc You will get information (dimension, variables and global attribute) about the data.","title":"Clip data using a bounding box based on area of interest"},{"location":"chirpsnc/#edit-variable-and-attribute","text":"As explain in Input specification , we can say from above picture there are few variable and attribute that need editing. Let's do it one-by-one. Navigate to Downloads/CHIRPS/netCDF/Clipped folder in the working directory. Variable Edit variable name for longitude to lon , and latitude to lat Paste and Enter line-by-line CDO script below in Terminal. cdo chname,longitude,lon java_chirps-v2.0.monthly.nc java_chirps-v2.0.monthly_1.nc cdo chname,latitude,lat java_chirps-v2.0.monthly_1.nc java_chirps-v2.0.monthly_2.nc Alternative, using NCO script is below ncrename -d longitude,lon -d latitude,lat -v longitude,lon -v latitude,lat java_chirps-v2.0.monthly.nc -O java_chirps-v2.0.monthly_2.nc Let's read header contents of above result. Type and execute below code: ncdump -h java_chirps-v2.0.monthly_2.nc Result from CDO Result from NCO Attribute Edit precipitation unit from mm/month to mm Paste and Enter line-by-line CDO script below in Terminal. cdo -setattribute,precip@units = \"mm\" java_chirps-v2.0.monthly_2.nc java_chirps-v2.0.monthly_3.nc Alternative, using NCO script is below ncatted -a units,precip,modify,c, 'mm' java_chirps-v2.0.monthly_2.nc java_chirps-v2.0.monthly_3.nc Let's read header contents of above result. Type and execute below code: ncdump -h java_chirps-v2.0.monthly_3.nc Result from CDO Result from NCO And the units already in mm Once this has completed, the dataset can be used as input to this package for computing SPI. From above picture, some of the precipitation attribute are still wrong: DimensionNames and Units . I can leave it as is, SPI code will only read units and variables precip(time,lat,lon) As the input data preparation is completed, move the file java_chirps-v2.0.monthly_3.nc to main folder Input_nc and rename into java_cli_imerg_1months_1981_2020.nc mv java_chirps-v2.0.monthly_3.nc ../../../../Input_nc/java_cli_imerg_1months_2000_2020.nc Make sure the file java_cli_imerg_1months_1981_2020.nc is available at Input_nc folder","title":"Edit variable and attribute"},{"location":"chirpstif/","text":"3.2. CHIRPS monthly in GeoTIFF format This section will explain on how to download CHIRPS monthly data in GeoTIFF format and prepare it as input for SPI calculation. Make sure you still inside conda gis environment Download CHIRPS data Navigate to Downloads/CHIRPS/GeoTIFF folder in the working directory. Download using wget all CHIRPS monthly data in GeoTIFF format from Jan 1981 to Dec 2020 (this is lot of data +-7GB zipped files, and become 27GB after extraction, please make sure you have bandwidth and unlimited data package). Paste and Enter below script in your Terminal. export URL = 'https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/tifs/' ; curl \" $URL \" | grep -E 'a href=' | perl -pe 's|.*href=\"(.*?)\".*|\\1|' | { while read -r f ; do wget \" $URL \" / \" $f \" ; done } Gunzip all the downloaded files gunzip *.gz Clip data using a shapefile based on area of interest Download the Java boundary shapefile https://github.com/wfpidn/SPI/blob/master/Data/Subset/java_bnd_chirps_subset.zip . And save it in Subset directory then unzip it. Info You can use your own boundary in shapefile and use it to clip the rainfall raster data based on your preferred area of interest. Still in your GeoTIFF directory, Clip your area of interest using Java boundary and save it to Input_TIF directory. I will use gdalwarp command from GDAL to clip all GeoTIFF files in a folder. for i in ` find *.tif ` ; do gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -srcnodata NoData -dstnodata -9999 -cutline ../../../Subset/java_bnd_chirps_subset.shp -crop_to_cutline $i ../../../Input_TIF/java_ $i ; done If you have limited data connection or lazy to download +-7GB and process +-27GB data, you can get pre-processed clipped data for Java covering Jan 1981 to Dec 2020, with file size +-6.8MB. Link: https://github.com/wfpidn/SPI/blob/master/Data/Input_TIF Convert GeoTIFFs to single netCDF Download python script/notebook that I use to convert GeoTIFF in a folder to single netCDF, save it to Script folder. Jupyter notebook Python script Below is the script #!/usr/bin/env python \"\"\" ------------------------------------------------------------------------------------------------------------- Convert CHIRPS GeoTIFF in a folder to single NetCDF file with time dimension enabled that is CF-Compliant http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html Based on Rich Signell's answer on StackExchange: https://gis.stackexchange.com/a/70487 This script was tested using CHIRPS dekad data. Adjustment is needed if using other timesteps data for CHIRPS NCO (http://nco.sourceforge.net) must be installed before using this script Modified by Benny Istanto, UN World Food Programme, benny.istanto@wfp.org ------------------------------------------------------------------------------------------------------------- \"\"\" # Case Java island, Indonesia - 105.05,116,25,-8.80,-5.05 # # Original data in GeoTIFF format downloaded from https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/tifs/ # Then clipped using Java boundary (http://on.istan.to/365PSyH) via gdalwarp # for i in `find *.tif`; do gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -srcnodata NoData -dstnodata -9999 -cutline java_bnd_chirps_subset.shp -crop_to_cutline $i java_$i; done # # Clipped GeoTIFF file for Java (https://on.istan.to/3iLu68v) import numpy as np import datetime as dt import os import gdal import netCDF4 import re ds = gdal . Open ( '/path/to/directory/java_chirps-v2.0.1981.01.tif' ) # Data location a = ds . ReadAsArray () nlat , nlon = np . shape ( a ) b = ds . GetGeoTransform () #bbox, interval lon = np . arange ( nlon ) * b [ 1 ] + b [ 0 ] lat = np . arange ( nlat ) * b [ 5 ] + b [ 3 ] basedate = dt . datetime ( 1980 , 1 , 1 , 0 , 0 , 0 ) # Create NetCDF file nco = netCDF4 . Dataset ( 'java_cli_chirps_1months_1981_2020.nc' , 'w' , clobber = True ) # Output name # Create dimensions, variables and attributes: nco . createDimension ( 'lon' , nlon ) nco . createDimension ( 'lat' , nlat ) nco . createDimension ( 'time' , None ) timeo = nco . createVariable ( 'time' , 'f4' ,( 'time' )) timeo . units = 'days since 1980-1-1 00:00:00' timeo . standard_name = 'time' timeo . calendar = 'gregorian' timeo . axis = 'T' lono = nco . createVariable ( 'lon' , 'f4' ,( 'lon' )) lono . units = 'degrees_east' lono . standard_name = 'longitude' lono . long_name = 'longitude' lono . axis = 'X' lato = nco . createVariable ( 'lat' , 'f4' ,( 'lat' )) lato . units = 'degrees_north' lato . standard_name = 'latitude' lato . long_name = 'latitude' lato . axis = 'Y' # Create container variable for CRS: lon/lat WGS84 datum crso = nco . createVariable ( 'crs' , 'i4' ) crso . long_name = 'Lon/Lat Coords in WGS84' crso . grid_mapping_name = 'latitude_longitude' crso . longitude_of_prime_meridian = 0.0 crso . semi_major_axis = 6378137.0 crso . inverse_flattening = 298.257223563 # Create float variable for precipitation data, with chunking pcpo = nco . createVariable ( 'precip' , 'f4' , ( 'time' , 'lat' , 'lon' ), zlib = True , fill_value =- 9999. ) pcpo . units = 'mm' pcpo . standard_name = 'convective precipitation rate' pcpo . long_name = 'Climate Hazards group InfraRed Precipitation with Stations' pcpo . time_step = 'dekad' pcpo . missing_value = - 9999. pcpo . geospatial_lat_min = - 8.8 pcpo . geospatial_lat_max = - 5.05 pcpo . geospatial_lon_min = 105.05 pcpo . geospatial_lon_max = 116.25 pcpo . grid_mapping = 'crs' pcpo . set_auto_maskandscale ( False ) # Additional attributes nco . Conventions = 'CF-1.6' nco . title = \"CHIRPS v2.0\" nco . history = \"created by Climate Hazards Group. University of California at Santa Barbara\" nco . version = \"Version 2.0\" nco . comments = \"time variable denotes the first day of the given dekad.\" nco . website = \"https://www.chc.ucsb.edu/data/chirps\" nco . date_created = \"2021-01-25\" nco . creator_name = \"Benny Istanto\" nco . creator_email = \"benny.istanto@wfp.org\" nco . institution = \"UN World Food Programme\" nco . note = \"The data is developed to support regular updating procedure for SPI analysis (https://github.com/wfpidn/SPI). This activities will support WFP to assess extreme dry and wet periods as part of WFP's Seasonal Monitoring\" # Write lon,lat lono [:] = lon lato [:] = lat pat = re . compile ( 'java_chirps-v2.0.[0-9] {4} \\.[0-9] {2} ' ) itime = 0 # Step through data, writing time and data to NetCDF for root , dirs , files in os . walk ( '/path/to/directory/' ): dirs . sort () files . sort () for f in files : if re . match ( pat , f ): # read the time values by parsing the filename year = int ( f [ 17 : 21 ]) mon = int ( f [ 22 : 24 ]) date = dt . datetime ( year , mon , 1 , 0 , 0 , 0 ) print ( date ) dtime = ( date - basedate ) . total_seconds () / 86400. timeo [ itime ] = dtime # precipitation pcp_path = os . path . join ( root , f ) print ( pcp_path ) pcp = gdal . Open ( pcp_path ) a = pcp . ReadAsArray () #data pcpo [ itime ,:,:] = a itime = itime + 1 nco . close () You MUST adjust the folder location (replace /path/to/directory/ with yours, example: /Users/bennyistanto/Temp/CHIRPS/SPI/Input_TIF/java_cli_chirps-v2.0.1981.01.1.tif ) in line 31 and 114. Warning If you are using other data source (I assume all the data in WGS84), you need to adjust few code in: Line 31: folder location Line 40: start of the date Line 44: output name Line 53: date attribute Line 85-88: bounding box Line 110: output filename structure Line 114: folder location Line 120-122: date character location in a filename After everything is set, then you can execute the translation process (choose one or you can try both for learning) Using Python in Terminal, navigate to your Script directory, type python tiff2nc.py Wait for a few moments, you will get the output java_cli_chirps_1months_1981_2020.nc . You will find this file inside Input_TIF folder. Move it to Input_nc folder. Using Jupyter, make sure you still inside conda gis environment. Access this *.ipynb file inside Script folder. Move it to Input_TIF folder. Navigate your Terminal to Input_TIF then type jupyter notebook Navigate to your notebook directory (where you put *.ipynb file), run Cell by Cell until completed. Wait for a few moments, you will get the output java_cli_chirps_1months_1981_2020.nc . As the input data preparation is completed, move the file java_cli_chirps_1months_1981_2020.nc to main folder Input_nc mv java_cli_chirps_1months_1981_2020.nc ../../../Input_nc/java_cli_imerg_1months_1981_2020.nc Make sure the file java_cli_chirps_1months_1981_2020.nc is available at Input_nc folder You also can get this data: java_cli_chirps_1months_1981_2020.nc via this link https://github.com/wfpidn/SPI/blob/master/Data/Input_nc/java_cli_chirps_1months_1981_2020.nc","title":"3.2. CHIRPS monthly GeoTIFF"},{"location":"chirpstif/#32-chirps-monthly-in-geotiff-format","text":"This section will explain on how to download CHIRPS monthly data in GeoTIFF format and prepare it as input for SPI calculation. Make sure you still inside conda gis environment","title":"3.2. CHIRPS monthly in GeoTIFF format"},{"location":"chirpstif/#download-chirps-data","text":"Navigate to Downloads/CHIRPS/GeoTIFF folder in the working directory. Download using wget all CHIRPS monthly data in GeoTIFF format from Jan 1981 to Dec 2020 (this is lot of data +-7GB zipped files, and become 27GB after extraction, please make sure you have bandwidth and unlimited data package). Paste and Enter below script in your Terminal. export URL = 'https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/tifs/' ; curl \" $URL \" | grep -E 'a href=' | perl -pe 's|.*href=\"(.*?)\".*|\\1|' | { while read -r f ; do wget \" $URL \" / \" $f \" ; done } Gunzip all the downloaded files gunzip *.gz","title":"Download CHIRPS data"},{"location":"chirpstif/#clip-data-using-a-shapefile-based-on-area-of-interest","text":"Download the Java boundary shapefile https://github.com/wfpidn/SPI/blob/master/Data/Subset/java_bnd_chirps_subset.zip . And save it in Subset directory then unzip it. Info You can use your own boundary in shapefile and use it to clip the rainfall raster data based on your preferred area of interest. Still in your GeoTIFF directory, Clip your area of interest using Java boundary and save it to Input_TIF directory. I will use gdalwarp command from GDAL to clip all GeoTIFF files in a folder. for i in ` find *.tif ` ; do gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -srcnodata NoData -dstnodata -9999 -cutline ../../../Subset/java_bnd_chirps_subset.shp -crop_to_cutline $i ../../../Input_TIF/java_ $i ; done If you have limited data connection or lazy to download +-7GB and process +-27GB data, you can get pre-processed clipped data for Java covering Jan 1981 to Dec 2020, with file size +-6.8MB. Link: https://github.com/wfpidn/SPI/blob/master/Data/Input_TIF","title":"Clip data using a shapefile based on area of interest"},{"location":"chirpstif/#convert-geotiffs-to-single-netcdf","text":"Download python script/notebook that I use to convert GeoTIFF in a folder to single netCDF, save it to Script folder. Jupyter notebook Python script Below is the script #!/usr/bin/env python \"\"\" ------------------------------------------------------------------------------------------------------------- Convert CHIRPS GeoTIFF in a folder to single NetCDF file with time dimension enabled that is CF-Compliant http://cfconventions.org/cf-conventions/v1.6.0/cf-conventions.html Based on Rich Signell's answer on StackExchange: https://gis.stackexchange.com/a/70487 This script was tested using CHIRPS dekad data. Adjustment is needed if using other timesteps data for CHIRPS NCO (http://nco.sourceforge.net) must be installed before using this script Modified by Benny Istanto, UN World Food Programme, benny.istanto@wfp.org ------------------------------------------------------------------------------------------------------------- \"\"\" # Case Java island, Indonesia - 105.05,116,25,-8.80,-5.05 # # Original data in GeoTIFF format downloaded from https://data.chc.ucsb.edu/products/CHIRPS-2.0/global_monthly/tifs/ # Then clipped using Java boundary (http://on.istan.to/365PSyH) via gdalwarp # for i in `find *.tif`; do gdalwarp --config GDALWARP_IGNORE_BAD_CUTLINE YES -srcnodata NoData -dstnodata -9999 -cutline java_bnd_chirps_subset.shp -crop_to_cutline $i java_$i; done # # Clipped GeoTIFF file for Java (https://on.istan.to/3iLu68v) import numpy as np import datetime as dt import os import gdal import netCDF4 import re ds = gdal . Open ( '/path/to/directory/java_chirps-v2.0.1981.01.tif' ) # Data location a = ds . ReadAsArray () nlat , nlon = np . shape ( a ) b = ds . GetGeoTransform () #bbox, interval lon = np . arange ( nlon ) * b [ 1 ] + b [ 0 ] lat = np . arange ( nlat ) * b [ 5 ] + b [ 3 ] basedate = dt . datetime ( 1980 , 1 , 1 , 0 , 0 , 0 ) # Create NetCDF file nco = netCDF4 . Dataset ( 'java_cli_chirps_1months_1981_2020.nc' , 'w' , clobber = True ) # Output name # Create dimensions, variables and attributes: nco . createDimension ( 'lon' , nlon ) nco . createDimension ( 'lat' , nlat ) nco . createDimension ( 'time' , None ) timeo = nco . createVariable ( 'time' , 'f4' ,( 'time' )) timeo . units = 'days since 1980-1-1 00:00:00' timeo . standard_name = 'time' timeo . calendar = 'gregorian' timeo . axis = 'T' lono = nco . createVariable ( 'lon' , 'f4' ,( 'lon' )) lono . units = 'degrees_east' lono . standard_name = 'longitude' lono . long_name = 'longitude' lono . axis = 'X' lato = nco . createVariable ( 'lat' , 'f4' ,( 'lat' )) lato . units = 'degrees_north' lato . standard_name = 'latitude' lato . long_name = 'latitude' lato . axis = 'Y' # Create container variable for CRS: lon/lat WGS84 datum crso = nco . createVariable ( 'crs' , 'i4' ) crso . long_name = 'Lon/Lat Coords in WGS84' crso . grid_mapping_name = 'latitude_longitude' crso . longitude_of_prime_meridian = 0.0 crso . semi_major_axis = 6378137.0 crso . inverse_flattening = 298.257223563 # Create float variable for precipitation data, with chunking pcpo = nco . createVariable ( 'precip' , 'f4' , ( 'time' , 'lat' , 'lon' ), zlib = True , fill_value =- 9999. ) pcpo . units = 'mm' pcpo . standard_name = 'convective precipitation rate' pcpo . long_name = 'Climate Hazards group InfraRed Precipitation with Stations' pcpo . time_step = 'dekad' pcpo . missing_value = - 9999. pcpo . geospatial_lat_min = - 8.8 pcpo . geospatial_lat_max = - 5.05 pcpo . geospatial_lon_min = 105.05 pcpo . geospatial_lon_max = 116.25 pcpo . grid_mapping = 'crs' pcpo . set_auto_maskandscale ( False ) # Additional attributes nco . Conventions = 'CF-1.6' nco . title = \"CHIRPS v2.0\" nco . history = \"created by Climate Hazards Group. University of California at Santa Barbara\" nco . version = \"Version 2.0\" nco . comments = \"time variable denotes the first day of the given dekad.\" nco . website = \"https://www.chc.ucsb.edu/data/chirps\" nco . date_created = \"2021-01-25\" nco . creator_name = \"Benny Istanto\" nco . creator_email = \"benny.istanto@wfp.org\" nco . institution = \"UN World Food Programme\" nco . note = \"The data is developed to support regular updating procedure for SPI analysis (https://github.com/wfpidn/SPI). This activities will support WFP to assess extreme dry and wet periods as part of WFP's Seasonal Monitoring\" # Write lon,lat lono [:] = lon lato [:] = lat pat = re . compile ( 'java_chirps-v2.0.[0-9] {4} \\.[0-9] {2} ' ) itime = 0 # Step through data, writing time and data to NetCDF for root , dirs , files in os . walk ( '/path/to/directory/' ): dirs . sort () files . sort () for f in files : if re . match ( pat , f ): # read the time values by parsing the filename year = int ( f [ 17 : 21 ]) mon = int ( f [ 22 : 24 ]) date = dt . datetime ( year , mon , 1 , 0 , 0 , 0 ) print ( date ) dtime = ( date - basedate ) . total_seconds () / 86400. timeo [ itime ] = dtime # precipitation pcp_path = os . path . join ( root , f ) print ( pcp_path ) pcp = gdal . Open ( pcp_path ) a = pcp . ReadAsArray () #data pcpo [ itime ,:,:] = a itime = itime + 1 nco . close () You MUST adjust the folder location (replace /path/to/directory/ with yours, example: /Users/bennyistanto/Temp/CHIRPS/SPI/Input_TIF/java_cli_chirps-v2.0.1981.01.1.tif ) in line 31 and 114. Warning If you are using other data source (I assume all the data in WGS84), you need to adjust few code in: Line 31: folder location Line 40: start of the date Line 44: output name Line 53: date attribute Line 85-88: bounding box Line 110: output filename structure Line 114: folder location Line 120-122: date character location in a filename After everything is set, then you can execute the translation process (choose one or you can try both for learning) Using Python in Terminal, navigate to your Script directory, type python tiff2nc.py Wait for a few moments, you will get the output java_cli_chirps_1months_1981_2020.nc . You will find this file inside Input_TIF folder. Move it to Input_nc folder. Using Jupyter, make sure you still inside conda gis environment. Access this *.ipynb file inside Script folder. Move it to Input_TIF folder. Navigate your Terminal to Input_TIF then type jupyter notebook Navigate to your notebook directory (where you put *.ipynb file), run Cell by Cell until completed. Wait for a few moments, you will get the output java_cli_chirps_1months_1981_2020.nc . As the input data preparation is completed, move the file java_cli_chirps_1months_1981_2020.nc to main folder Input_nc mv java_cli_chirps_1months_1981_2020.nc ../../../Input_nc/java_cli_imerg_1months_1981_2020.nc Make sure the file java_cli_chirps_1months_1981_2020.nc is available at Input_nc folder You also can get this data: java_cli_chirps_1months_1981_2020.nc via this link https://github.com/wfpidn/SPI/blob/master/Data/Input_nc/java_cli_chirps_1months_1981_2020.nc","title":"Convert GeoTIFFs to single netCDF"},{"location":"conversion/","text":"7. Convert the result to GeoTIFF Warning Below guideline using CHIRPS data as example. If you are using IMERG, just replace text CHIRPS in the script or filename below with IMERG. I need CDO to do a conversion of the result into GeoTIFF format, and CDO required the variable should be in time , lat , lon , while the output from SPI: java_xxxxx_spi_xxxxx_x_month.nc in lat , lon , time , you can check this via ncdump -h java_CHIRPS_spi_gamma_3_month.nc Deactivate an active environment climate_indices then activate environment gis to start working on output conversion using CDO and GDAL. conda deactivate && conda activate gis Navigate your Terminal to folder Output_nc Let's re-order the variables into time , lat , lon using ncpdq command from NCO and save the result to folder Output_TEMP ncpdq -a time,lat,lon java_CHIRPS_spi_gamma_3_month.nc ../Output_TEMP/java_CHIRPS_spi_gamma_3_month_rev.nc Navigate your Terminal to folder Output_TEMP Check result and metadata to make sure everything is correct. ncdump -h java_CHIRPS_spi_gamma_3_month_rev.nc Then convert all SPI value into GeoTIFF with time dimension information as the filename using CDO and GDAL. Navigate your Terminal to folder Output_TEMP , execute below script and save the result to folder Output_TIF for t in ` cdo showdate java_CHIRPS_spi_gamma_3_month_rev.nc ` ; do cdo seldate, $t java_CHIRPS_spi_gamma_3_month_rev.nc dummy.nc gdal_translate -of GTiff -a_ullr 105 .05 -5.05 116 .25 -8.8 -a_srs EPSG:4326 -co COMPRESS = LZW -co PREDICTOR = 1 dummy.nc ../Output_TIF/java_cli_chirps-v2.0. $t .spi3.tif done Note If you are using IMERG or other data, or different area of interest, please make sure check the bounding box of the data <ulx> <uly> <lrx> <lry> . This -a_ullr assigns georeferenced bounds to the output file, ignoring what would have been derived from the source file. FINISH Congrats, now you are able to calculate SPI based on monthly rainfall in netCDF or GeoTIFF format.","title":"7. Convert the result to GeoTIFF"},{"location":"conversion/#7-convert-the-result-to-geotiff","text":"Warning Below guideline using CHIRPS data as example. If you are using IMERG, just replace text CHIRPS in the script or filename below with IMERG. I need CDO to do a conversion of the result into GeoTIFF format, and CDO required the variable should be in time , lat , lon , while the output from SPI: java_xxxxx_spi_xxxxx_x_month.nc in lat , lon , time , you can check this via ncdump -h java_CHIRPS_spi_gamma_3_month.nc Deactivate an active environment climate_indices then activate environment gis to start working on output conversion using CDO and GDAL. conda deactivate && conda activate gis Navigate your Terminal to folder Output_nc Let's re-order the variables into time , lat , lon using ncpdq command from NCO and save the result to folder Output_TEMP ncpdq -a time,lat,lon java_CHIRPS_spi_gamma_3_month.nc ../Output_TEMP/java_CHIRPS_spi_gamma_3_month_rev.nc Navigate your Terminal to folder Output_TEMP Check result and metadata to make sure everything is correct. ncdump -h java_CHIRPS_spi_gamma_3_month_rev.nc Then convert all SPI value into GeoTIFF with time dimension information as the filename using CDO and GDAL. Navigate your Terminal to folder Output_TEMP , execute below script and save the result to folder Output_TIF for t in ` cdo showdate java_CHIRPS_spi_gamma_3_month_rev.nc ` ; do cdo seldate, $t java_CHIRPS_spi_gamma_3_month_rev.nc dummy.nc gdal_translate -of GTiff -a_ullr 105 .05 -5.05 116 .25 -8.8 -a_srs EPSG:4326 -co COMPRESS = LZW -co PREDICTOR = 1 dummy.nc ../Output_TIF/java_cli_chirps-v2.0. $t .spi3.tif done Note If you are using IMERG or other data, or different area of interest, please make sure check the bounding box of the data <ulx> <uly> <lrx> <lry> . This -a_ullr assigns georeferenced bounds to the output file, ignoring what would have been derived from the source file. FINISH Congrats, now you are able to calculate SPI based on monthly rainfall in netCDF or GeoTIFF format.","title":"7. Convert the result to GeoTIFF"},{"location":"directory/","text":"0. Working Directory For this tutorial, I am working on these folder /Users/bennyistanto/Temp/xxx/SPI/Java (applied to Mac/Linux machine) or Z:/Temp/xxx/SPI/Java (applied to Windows machine) directory. I have some folder inside this directory: Downloads IMERG IMERG_mmhr IMERG_mmmonth IMERG_originalfiles CHIRPS netCDF GeoTIFF Place to put downloaded IMERG or CHIRPS data Fitting Place to put fitting parameters output from the calculation Input_nc Place to put netCDF data that will use as an input Input_TIF Place to put GeoTIFF file that will use to convert to single netCDF with time dimension enabled Output_nc Output folder for SPI calculation Output_TEMP Temporary for nc files from CDO arrange dimension process Output_TIF Final output of SPI, generate by CDO and GDAL Script Python script location to convert bunch of GeoTIFF file to single netCDF Subset Place to put shapefile for clipping area of interest Feel free to use your own preferences for this setting/folder arrangements. Notes: xxx = IMERG or CHIRPS (depend on the data used in tutorial)","title":"0. Working directory"},{"location":"directory/#0-working-directory","text":"For this tutorial, I am working on these folder /Users/bennyistanto/Temp/xxx/SPI/Java (applied to Mac/Linux machine) or Z:/Temp/xxx/SPI/Java (applied to Windows machine) directory. I have some folder inside this directory: Downloads IMERG IMERG_mmhr IMERG_mmmonth IMERG_originalfiles CHIRPS netCDF GeoTIFF Place to put downloaded IMERG or CHIRPS data Fitting Place to put fitting parameters output from the calculation Input_nc Place to put netCDF data that will use as an input Input_TIF Place to put GeoTIFF file that will use to convert to single netCDF with time dimension enabled Output_nc Output folder for SPI calculation Output_TEMP Temporary for nc files from CDO arrange dimension process Output_TIF Final output of SPI, generate by CDO and GDAL Script Python script location to convert bunch of GeoTIFF file to single netCDF Subset Place to put shapefile for clipping area of interest Feel free to use your own preferences for this setting/folder arrangements. Notes: xxx = IMERG or CHIRPS (depend on the data used in tutorial)","title":"0. Working Directory"},{"location":"imergnc/","text":"3.1. IMERG monthly in netCDF format Following the guideline from NASA ARSET, this section will explain on how to download IMERG monthly data in netCDF format and prepare it as input for SPI calculation. Download monthly IMERG data from GES DISC Using a web browser, go to NASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC): https://disc.gsfc.nasa.gov/ Type \u201cIMERG\u201d in the search bar and click on the search Select IMERG Version 6 Level 3 data at \u201cmonthly\u201d temporal resolution and click on the \u201cSubset/Get Data\u201d icon Current latest data is up to Apr 2021, but for this guideline I will download for period Jun 2000 - Dec 2021 Under Spatial Subset enter 105.05, -8.8, 116.25, -5.05 This spatial subset is for Java island, Indonesia Under Variables select only precipitation Leave the default parameters under Grid Under File Format select \"netCDF\" Click Get Data Data links windows will popup and you may click \" Download links list \" You will get a txt file with similar filename like this one subset_GPM_3IMERGM_06_20210707_044656.txt Move this file into your working directory (in this case I have folder /Download/IMERG/IMERG_originalfiles to save the txt file) Navigate your terminal to folder /Downloads/IMERG/IMERG_originalfiles and type this code to download the data: wget -c -i subset_GPM_3IMERGM_06_20200703_065511.txt ` If you are lazy to follow the process of downloading data, for convenience these data are made available on via this link: https://github.com/wfpidn/SPI/blob/master/Data/Downloads/IMERG/IMERG_originalfiles.zip Once downloaded, unzip IMERG_originalfiles.zip Rename all the data into friendly filename If you check the data in folder IMERG_originalfiles , you will find the data with filename something like HTTP_services.cgi?FILENAME=%2Fdata%2FGPM_L3%2FGPM_3IMERGM.06%2F2000%2F3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5&FORMAT=bmM0Lw&BBOX=-8.8,105.05,-5.05,116.25&LABEL=3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 I need to rename it all the file into friendly filename like this 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 If you follow the download process, you may create a duplicate for contents in IMERG_originalfiles to IMERG_mmhr (just in case something happen to your downloaded files). But if you are not follow the download process but downloaded IMERG_originalfiles.zip folder, you are good. I will use regular expression and remove the first 178 characters in the filename (I will remove text HTTP_services.cgi?FILENAME=%2Fdata%2FGPM_L3%2FGPM_3IMERGM.06%2F2000%2F3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5&FORMAT=bmM0Lw&BBOX=-8.8,105.05,-5.05,116.25&LABEL= and leaving 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 ). Using rename command, make sure you are navigate to IMERG_mmhr directory in your terminal, type below code: rename 's/.{178}//g' *.nc4 ` And below is the result! Convert unit from mm/hr to mm/month Make sure you are inside gis environment and IMERG_mmhr folder, I will use NCO to pre-process the data. Let's read header contents of a netCDF file in IMERG_mmhr folder. I will use this data 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 as example. Type and execute below code: ncdump -h 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 You will get information (dimension, variables and global attribute) about the data. As you can see above picture, the original downloaded files unit from GPM IMERG is in mm/hr , while to calculate monthly SPI, the data must be in mm/month . I need to do a conversion process using ncap2 (arithmatic operator for netCDF files) command by multiplying number of day in month with 24hour . Example: Rainfall value in Month: JAN , MAR , MAY , JUL , AUG , OCT , DEC which has 31 days will multiply with 744 to get mm/month Rainfall value in Month: APR , JUN , SEP , NOV which has 30 days will multiply with 720 to get mm/month Rainfall value in Month: FEB in a leap year which has 29 days will multiply with 696 to get mm/month Rainfall value in Month: FEB in a normal year which has 28 days will multiply with 672 to get mm/month To do the calculation, I will use below script to help generate line of codes for converting value of each data from mm/hr to mm/month while read -r _file ; do file = $( basename -- \" $_file \" ) yearmonth = $( echo \" $file \" | sed -E 's/.*\\.3IMERG\\.([0-9]{6})[0-9]{2}-.*/\\1/' ) mult = $( python - \" $yearmonth \" <<EOF import sys, calendar ym = sys.argv[1] print(calendar.monthrange(int(ym[:4]), int(ym[4:]))[1] * 24) EOF ) ; echo ncap2 -s 'precipitation=' \" $mult \" '*precipitation' \" $file \" ../IMERG_mmmonth/ \" $file \" ; done < < ( find . -maxdepth 1 -type f -name \"*.nc4\" ) > script.sh Paste above code in your Terminal and Enter. You will get a file named script.sh as the result. Then execute below sh script.sh All file inside IMERG_mmmonth will have rainfall which show the value in mm/month . Let's check file 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 in folder IMERG_mmhr and IMERG_mmmonth using Panoply, see the difference in range of value. Monthly rainfall in mm/hr Monthly rainfall in mm/month I am aware the unit text still in mm/hr , I will explain how to edit it in the next topic. Create single netCDF file Navigate to IMERG_mmmonth folder in Terminal. Loop all files in the folder IMERG_mmmonth to make time the record dimension/variable used for concatenating files using ncks command for fl in *.nc4 ; do ncks -O --mk_rec_dmn time $fl $fl ; done Concatenates all nc4 files in IMERG_mmmonth folder into one nc4 file named IMERG_concat.nc4 using ncrcat command ncrcat -h *.nc4 IMERG_concat.nc4 Check the header ncdump -h IMERG_concat.nc4 And the variables for precipitation is time , lon , lat but SPI calculation required: lat , lon , time or time , lat , lon Let's re-order the variables into time , lat , lon using ncpdq command, to be able running the SPI code in Python ncpdq -a time,lat,lon IMERG_concat.nc4 IMERG_concat_ncpdq0.nc4 Check again the header for the result IMERG_concat_ncpdq0.nc4 ncdump -h IMERG_concat_ncpdq0.nc4 And the variables for precipitation is time , lat , lon , it means the result is correct. But the unit still in mm/hr . Warning Notes on re-ordering process (Case by case) After re-ordering the variables, sometimes user experience lat or lon dimension becomes UNLIMITED which is wrong. The time dimension should be the UNLIMITED dimension. Fortunately you can do this to fix the lat or lon dimension who becomes UNLIMITED using ncks command below: ncks --fix_rec_dmn lat IMERG_concat_ncpdq0.nc4 -o outfixed.nc4 ; mv outfixed.nc4 IMERG_concat_ncpdq0.nc4 And to make UNLIMITED the time dimension again using ncks command below: ncks --mk_rec_dmn time IMERG_concat_ncpdq0.nc4 -o outfunlim.nc4 ; mv outunlim.nc4 IMERG_concat_ncpdq0.nc4 If you don't come accross the problem, lat or lon dimension becomes UNLIMITED , then skip above process and go directly to step below. SPI code does not recognized unit mm/hr or mm/month , I need to edit into mm . To edit the unit attribute names, I will use ncatted command, follow below code. ncatted -a units,precipitation,modify,c, 'mm' IMERG_concat_ncpdq0.nc4 IMERG_concat_ncpdq1.nc4 Check again the header for IMERG_concat_ncpdq1.nc4 , to make sure everything is correct. ncdump -h IMERG_concat_ncpdq1.nc4 And the units already in mm Once this has completed, the dataset can be used as input to this package for computing SPI. From above picture, some of the precipitation attribute are still wrong: DimensionNames and Units . I can leave it as is, SPI code will only read units and variables precipitation(time,lat,lon) As the input data preparation is completed, move the file IMERG_concat_ncpdq1.nc4 to main folder Input_nc and rename into java_cli_imerg_1months_2000_2020.nc mv IMERG_concat_ncpdq1.nc4 ../../../Input_nc/java_cli_imerg_1months_2000_2020.nc Make sure the file java_cli_imerg_1months_2000_2020.nc is available at Input_nc folder","title":"3.1 IMERG monthly netCDF"},{"location":"imergnc/#31-imerg-monthly-in-netcdf-format","text":"Following the guideline from NASA ARSET, this section will explain on how to download IMERG monthly data in netCDF format and prepare it as input for SPI calculation.","title":"3.1. IMERG monthly in netCDF format"},{"location":"imergnc/#download-monthly-imerg-data-from-ges-disc","text":"Using a web browser, go to NASA Goddard Earth Sciences (GES) Data and Information Services Center (DISC): https://disc.gsfc.nasa.gov/ Type \u201cIMERG\u201d in the search bar and click on the search Select IMERG Version 6 Level 3 data at \u201cmonthly\u201d temporal resolution and click on the \u201cSubset/Get Data\u201d icon Current latest data is up to Apr 2021, but for this guideline I will download for period Jun 2000 - Dec 2021 Under Spatial Subset enter 105.05, -8.8, 116.25, -5.05 This spatial subset is for Java island, Indonesia Under Variables select only precipitation Leave the default parameters under Grid Under File Format select \"netCDF\" Click Get Data Data links windows will popup and you may click \" Download links list \" You will get a txt file with similar filename like this one subset_GPM_3IMERGM_06_20210707_044656.txt Move this file into your working directory (in this case I have folder /Download/IMERG/IMERG_originalfiles to save the txt file) Navigate your terminal to folder /Downloads/IMERG/IMERG_originalfiles and type this code to download the data: wget -c -i subset_GPM_3IMERGM_06_20200703_065511.txt ` If you are lazy to follow the process of downloading data, for convenience these data are made available on via this link: https://github.com/wfpidn/SPI/blob/master/Data/Downloads/IMERG/IMERG_originalfiles.zip Once downloaded, unzip IMERG_originalfiles.zip","title":"Download monthly IMERG data from GES DISC"},{"location":"imergnc/#rename-all-the-data-into-friendly-filename","text":"If you check the data in folder IMERG_originalfiles , you will find the data with filename something like HTTP_services.cgi?FILENAME=%2Fdata%2FGPM_L3%2FGPM_3IMERGM.06%2F2000%2F3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5&FORMAT=bmM0Lw&BBOX=-8.8,105.05,-5.05,116.25&LABEL=3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 I need to rename it all the file into friendly filename like this 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 If you follow the download process, you may create a duplicate for contents in IMERG_originalfiles to IMERG_mmhr (just in case something happen to your downloaded files). But if you are not follow the download process but downloaded IMERG_originalfiles.zip folder, you are good. I will use regular expression and remove the first 178 characters in the filename (I will remove text HTTP_services.cgi?FILENAME=%2Fdata%2FGPM_L3%2FGPM_3IMERGM.06%2F2000%2F3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5&FORMAT=bmM0Lw&BBOX=-8.8,105.05,-5.05,116.25&LABEL= and leaving 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 ). Using rename command, make sure you are navigate to IMERG_mmhr directory in your terminal, type below code: rename 's/.{178}//g' *.nc4 ` And below is the result!","title":"Rename all the data into friendly filename"},{"location":"imergnc/#convert-unit-from-mmhr-to-mmmonth","text":"Make sure you are inside gis environment and IMERG_mmhr folder, I will use NCO to pre-process the data. Let's read header contents of a netCDF file in IMERG_mmhr folder. I will use this data 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 as example. Type and execute below code: ncdump -h 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 You will get information (dimension, variables and global attribute) about the data. As you can see above picture, the original downloaded files unit from GPM IMERG is in mm/hr , while to calculate monthly SPI, the data must be in mm/month . I need to do a conversion process using ncap2 (arithmatic operator for netCDF files) command by multiplying number of day in month with 24hour . Example: Rainfall value in Month: JAN , MAR , MAY , JUL , AUG , OCT , DEC which has 31 days will multiply with 744 to get mm/month Rainfall value in Month: APR , JUN , SEP , NOV which has 30 days will multiply with 720 to get mm/month Rainfall value in Month: FEB in a leap year which has 29 days will multiply with 696 to get mm/month Rainfall value in Month: FEB in a normal year which has 28 days will multiply with 672 to get mm/month To do the calculation, I will use below script to help generate line of codes for converting value of each data from mm/hr to mm/month while read -r _file ; do file = $( basename -- \" $_file \" ) yearmonth = $( echo \" $file \" | sed -E 's/.*\\.3IMERG\\.([0-9]{6})[0-9]{2}-.*/\\1/' ) mult = $( python - \" $yearmonth \" <<EOF import sys, calendar ym = sys.argv[1] print(calendar.monthrange(int(ym[:4]), int(ym[4:]))[1] * 24) EOF ) ; echo ncap2 -s 'precipitation=' \" $mult \" '*precipitation' \" $file \" ../IMERG_mmmonth/ \" $file \" ; done < < ( find . -maxdepth 1 -type f -name \"*.nc4\" ) > script.sh Paste above code in your Terminal and Enter. You will get a file named script.sh as the result. Then execute below sh script.sh All file inside IMERG_mmmonth will have rainfall which show the value in mm/month . Let's check file 3B-MO.MS.MRG.3IMERG.20000601-S000000-E235959.06.V06B.HDF5.nc4 in folder IMERG_mmhr and IMERG_mmmonth using Panoply, see the difference in range of value. Monthly rainfall in mm/hr Monthly rainfall in mm/month I am aware the unit text still in mm/hr , I will explain how to edit it in the next topic.","title":"Convert unit from mm/hr to mm/month"},{"location":"imergnc/#create-single-netcdf-file","text":"Navigate to IMERG_mmmonth folder in Terminal. Loop all files in the folder IMERG_mmmonth to make time the record dimension/variable used for concatenating files using ncks command for fl in *.nc4 ; do ncks -O --mk_rec_dmn time $fl $fl ; done Concatenates all nc4 files in IMERG_mmmonth folder into one nc4 file named IMERG_concat.nc4 using ncrcat command ncrcat -h *.nc4 IMERG_concat.nc4 Check the header ncdump -h IMERG_concat.nc4 And the variables for precipitation is time , lon , lat but SPI calculation required: lat , lon , time or time , lat , lon Let's re-order the variables into time , lat , lon using ncpdq command, to be able running the SPI code in Python ncpdq -a time,lat,lon IMERG_concat.nc4 IMERG_concat_ncpdq0.nc4 Check again the header for the result IMERG_concat_ncpdq0.nc4 ncdump -h IMERG_concat_ncpdq0.nc4 And the variables for precipitation is time , lat , lon , it means the result is correct. But the unit still in mm/hr . Warning Notes on re-ordering process (Case by case) After re-ordering the variables, sometimes user experience lat or lon dimension becomes UNLIMITED which is wrong. The time dimension should be the UNLIMITED dimension. Fortunately you can do this to fix the lat or lon dimension who becomes UNLIMITED using ncks command below: ncks --fix_rec_dmn lat IMERG_concat_ncpdq0.nc4 -o outfixed.nc4 ; mv outfixed.nc4 IMERG_concat_ncpdq0.nc4 And to make UNLIMITED the time dimension again using ncks command below: ncks --mk_rec_dmn time IMERG_concat_ncpdq0.nc4 -o outfunlim.nc4 ; mv outunlim.nc4 IMERG_concat_ncpdq0.nc4 If you don't come accross the problem, lat or lon dimension becomes UNLIMITED , then skip above process and go directly to step below. SPI code does not recognized unit mm/hr or mm/month , I need to edit into mm . To edit the unit attribute names, I will use ncatted command, follow below code. ncatted -a units,precipitation,modify,c, 'mm' IMERG_concat_ncpdq0.nc4 IMERG_concat_ncpdq1.nc4 Check again the header for IMERG_concat_ncpdq1.nc4 , to make sure everything is correct. ncdump -h IMERG_concat_ncpdq1.nc4 And the units already in mm Once this has completed, the dataset can be used as input to this package for computing SPI. From above picture, some of the precipitation attribute are still wrong: DimensionNames and Units . I can leave it as is, SPI code will only read units and variables precipitation(time,lat,lon) As the input data preparation is completed, move the file IMERG_concat_ncpdq1.nc4 to main folder Input_nc and rename into java_cli_imerg_1months_2000_2020.nc mv IMERG_concat_ncpdq1.nc4 ../../../Input_nc/java_cli_imerg_1months_2000_2020.nc Make sure the file java_cli_imerg_1months_2000_2020.nc is available at Input_nc folder","title":"Create single netCDF file"},{"location":"output/","text":"Example output Below is the example of SPI calculation using IMERG and CHIRPS data, SPI 3-month, March 2020. IMERG CHIRPS And comparison with SPI generated by: BMKG Indonesia Meteorological, Climatological and Geophysical Agency ( BMKG ) regularly produce SPI-3 and published on their website every month https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg Link for SPI 3-month, March 2020 - https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg?p=the-standardized-precipitation-index-maret-2020&tag=spi&lang=ID Climate Engine Climate Engine is a free web application powered by Google Earth Engine that can be used to create on-demand maps and charts from publicly available satellite and climate data using a standard web browser. Climate Engine allows users to analyze and interact with climate and earth observations for decision support related to drought, water use, agricultural, wildfire, and ecology. One of the product that could generate easily using Climate Engine is SPI. Link https://climengine.page.link/nTyi","title":"Example"},{"location":"output/#example-output","text":"Below is the example of SPI calculation using IMERG and CHIRPS data, SPI 3-month, March 2020.","title":"Example output"},{"location":"output/#imerg","text":"","title":"IMERG"},{"location":"output/#chirps","text":"And comparison with SPI generated by:","title":"CHIRPS"},{"location":"output/#bmkg","text":"Indonesia Meteorological, Climatological and Geophysical Agency ( BMKG ) regularly produce SPI-3 and published on their website every month https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg Link for SPI 3-month, March 2020 - https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg?p=the-standardized-precipitation-index-maret-2020&tag=spi&lang=ID","title":"BMKG"},{"location":"output/#climate-engine","text":"Climate Engine is a free web application powered by Google Earth Engine that can be used to create on-demand maps and charts from publicly available satellite and climate data using a standard web browser. Climate Engine allows users to analyze and interact with climate and earth observations for decision support related to drought, water use, agricultural, wildfire, and ecology. One of the product that could generate easily using Climate Engine is SPI. Link https://climengine.page.link/nTyi","title":"Climate Engine"},{"location":"pyenv/","text":"2. Configure the python environment The code for calculating SPI is written in Python 3. It is recommended to use either the Miniconda3 (minimal Anaconda) or Anaconda3 distribution. The below instructions will be Anaconda specific (although relevant to any Python virtual environment), and assume the use of a bash shell. A new Anaconda environment can be created using the conda environment management system that comes packaged with Anaconda. In the following examples, I\u2019ll use an environment named climate_indices (any environment name can be used instead of climate_indices ) which will be created and populated with all required dependencies through the use of the provided setup.py file. Note This step must only be done the first time . Once the environment has been created there is no need to do it again. First, open your Terminal (in your macOS/Linux and Ubuntu Linux on WSL), create the Python environment with python3.7 as default: conda create -n climate_indices python = 3 .7 Proceed with y The environment created can now be \u2018activated\u2019: conda activate climate_indices Install climate-indices package. Once the environment has been activated then subsequent Python commands will run in this environment where the package dependencies for this project are present. Now the package can be added to the environment along with all required modules (dependencies) via pip : pip install climate-indices Install netCDF Operator ( NCO ) using conda and proceed with y . conda install -c conda-forge nco Install Climate Data Operator ( CDO ) from Max-Planck-Institut f\u00fcr Meteorologie using conda and proceed with y . conda install -c conda-forge cdo Install jupyter and other package using conda and proceed with y . conda install -c conda-forge jupyter numpy netCDF4 Deactivate an active environment climate_indices as I will create a new environment called gis to install gdal to clip the rainfall data using a shapefile, and proceed with y . conda deactivate && conda create -n gis python = 3 .7 The environment created can now be \u2018activated\u2019: conda activate gis Install gdal , nco and cdo in gis environment and proceed with y . conda install -c conda-forge gdal nco cdo wget","title":"2. Configure the python environment"},{"location":"pyenv/#2-configure-the-python-environment","text":"The code for calculating SPI is written in Python 3. It is recommended to use either the Miniconda3 (minimal Anaconda) or Anaconda3 distribution. The below instructions will be Anaconda specific (although relevant to any Python virtual environment), and assume the use of a bash shell. A new Anaconda environment can be created using the conda environment management system that comes packaged with Anaconda. In the following examples, I\u2019ll use an environment named climate_indices (any environment name can be used instead of climate_indices ) which will be created and populated with all required dependencies through the use of the provided setup.py file. Note This step must only be done the first time . Once the environment has been created there is no need to do it again. First, open your Terminal (in your macOS/Linux and Ubuntu Linux on WSL), create the Python environment with python3.7 as default: conda create -n climate_indices python = 3 .7 Proceed with y The environment created can now be \u2018activated\u2019: conda activate climate_indices Install climate-indices package. Once the environment has been activated then subsequent Python commands will run in this environment where the package dependencies for this project are present. Now the package can be added to the environment along with all required modules (dependencies) via pip : pip install climate-indices Install netCDF Operator ( NCO ) using conda and proceed with y . conda install -c conda-forge nco Install Climate Data Operator ( CDO ) from Max-Planck-Institut f\u00fcr Meteorologie using conda and proceed with y . conda install -c conda-forge cdo Install jupyter and other package using conda and proceed with y . conda install -c conda-forge jupyter numpy netCDF4 Deactivate an active environment climate_indices as I will create a new environment called gis to install gdal to clip the rainfall data using a shapefile, and proceed with y . conda deactivate && conda create -n gis python = 3 .7 The environment created can now be \u2018activated\u2019: conda activate gis Install gdal , nco and cdo in gis environment and proceed with y . conda install -c conda-forge gdal nco cdo wget","title":"2. Configure the python environment"},{"location":"rainfall/","text":"3. Preparing input for SPI SPI requires monthly rainfall data, and there are many source providing global high-resolution gridded monthly rainfall data: CHIRPS IMERG FLDAS TerraClimate CRU For better result, SPI required minimum 30-years of data. I provide 3 different approach to prepare rainfall data ready to use as SPI input. For learning matters, you may follow all the approach. Or you can choose which one is suit for you depend on the data source and format: IMERG monthly in netCDF CHIRPS monthly in GeoTIFF CHIRPS monthly in netCDF Why I chosed CHIRPS as an alternative example from IMERG? It is produced at 0.05 x 0.05 degree spatial resolution, make CHIRPS the highest gridded rainfall data, and long-term historical data from 1981 \u2013 now. If you are prefer to use your own dataset also fine, you can still follow this guideline and adjust some steps and code related to filename, unit, format and structure. Input specification The climate-indices python package enables the user to calculate SPI using any gridded netCDF dataset. However, there are certain specifications for input files that vary based on input type. Precipitation unit must be written as millimeters , milimeter , mm , inches , inch or in . Data dimension and order must be written as lat , lon , time (Windows machine required this order) or time , lat , lon (Works tested on Mac/Linux and Linux running on WSL).","title":"3. Preparing input for SPI"},{"location":"rainfall/#3-preparing-input-for-spi","text":"SPI requires monthly rainfall data, and there are many source providing global high-resolution gridded monthly rainfall data: CHIRPS IMERG FLDAS TerraClimate CRU For better result, SPI required minimum 30-years of data. I provide 3 different approach to prepare rainfall data ready to use as SPI input. For learning matters, you may follow all the approach. Or you can choose which one is suit for you depend on the data source and format: IMERG monthly in netCDF CHIRPS monthly in GeoTIFF CHIRPS monthly in netCDF Why I chosed CHIRPS as an alternative example from IMERG? It is produced at 0.05 x 0.05 degree spatial resolution, make CHIRPS the highest gridded rainfall data, and long-term historical data from 1981 \u2013 now. If you are prefer to use your own dataset also fine, you can still follow this guideline and adjust some steps and code related to filename, unit, format and structure.","title":"3. Preparing input for SPI"},{"location":"rainfall/#input-specification","text":"The climate-indices python package enables the user to calculate SPI using any gridded netCDF dataset. However, there are certain specifications for input files that vary based on input type. Precipitation unit must be written as millimeters , milimeter , mm , inches , inch or in . Data dimension and order must be written as lat , lon , time (Windows machine required this order) or time , lat , lon (Works tested on Mac/Linux and Linux running on WSL).","title":"Input specification"},{"location":"references/","text":"References Journal Guttman, N. B., 1999: Accepting the Standardized Precipitation Index: A calculation algorithm. J. Amer. Water Resour. Assoc., 35(2), 311-322. Link Lloyd Hughes, B., and M. A. Saunders, 2002: A drought climatology for Europe. Int. J. Climatol., DOI:10.1002/joc.846 Link McKee, T.B., N. J. Doesken, and J. Kliest, 1993: The relationship of drought frequency and duration to time scales. In Proceedings of the 8th Conference of Applied Climatology, 17 22 January, Anaheim, CA. American Meterological Society, Boston, MA. 179-18. Link Guttman, N. B., 1998: Comparing the Palmer Drought Index and the Standardized Precipitation Index. J. Amer. Water Resources Assoc., 34(1), 113-121. Edwards, D. C., and T. B. McKee, 1997: Characteristics of 20th century drought in the United States at multiple time scales. Climatology Report No. 97-2, Colorado State Univ., Ft. Collins, CO. McKee, T. B., N. J. Doesken, and J. Kleist, 1995: Drought monitoring with multiple time scales. Ninth Conference on Applied Climatology, American Meteorological Society, Jan15-20, 1995, Dallas TX, pp.233-236. Website Keyantash, John & National Center for Atmospheric Research Staff (Eds). \"The Climate Data Guide: Standardized Precipitation Index (SPI).\" Retrieved from https://climatedataguide.ucar.edu/climate-data/standardized-precipitation-index-spi National Drought Mitigation Center (NDMC) at the University of Nebraska Lincoln. Link World Meteorological Organization (WMO), 2012: Standardized Precipitation Index User Guide. Link Climate Indices in Python https://climate-indices.readthedocs.io/en/latest/ Climate Engine - https://app.climateengine.org/climateEngine BMKG - https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg NASA ARSET on Application of GPM IMERG Reanalysis for Assessing Extreme Dry and Wet Periods. Link Climate Data Operator - https://code.mpimet.mpg.de/projects/cdo netCDF Operator - http://nco.sourceforge.net CHIRPS, Climate Hazard Centre UCSB - https://chc.ucsb.edu/data/chirps IMERG, Integrated Multi-satellitE Retrievals for GPM - https://gpm.nasa.gov/data/imerg https://benny.istan.to/blog/20200706-calculate-spi-using-imerg-data https://benny.istan.to/blog/20200710-calculate-spi-using-chirps-data https://benny.istan.to/blog/20201210-geotiff-to-netcdf-file-with-time-dimension-enabled-and-cf-compliant https://benny.istan.to/blog/20210125-calculate-spi-using-monthly-rainfall-data-in-geotiff-format","title":"References"},{"location":"references/#references","text":"","title":"References"},{"location":"references/#journal","text":"Guttman, N. B., 1999: Accepting the Standardized Precipitation Index: A calculation algorithm. J. Amer. Water Resour. Assoc., 35(2), 311-322. Link Lloyd Hughes, B., and M. A. Saunders, 2002: A drought climatology for Europe. Int. J. Climatol., DOI:10.1002/joc.846 Link McKee, T.B., N. J. Doesken, and J. Kliest, 1993: The relationship of drought frequency and duration to time scales. In Proceedings of the 8th Conference of Applied Climatology, 17 22 January, Anaheim, CA. American Meterological Society, Boston, MA. 179-18. Link Guttman, N. B., 1998: Comparing the Palmer Drought Index and the Standardized Precipitation Index. J. Amer. Water Resources Assoc., 34(1), 113-121. Edwards, D. C., and T. B. McKee, 1997: Characteristics of 20th century drought in the United States at multiple time scales. Climatology Report No. 97-2, Colorado State Univ., Ft. Collins, CO. McKee, T. B., N. J. Doesken, and J. Kleist, 1995: Drought monitoring with multiple time scales. Ninth Conference on Applied Climatology, American Meteorological Society, Jan15-20, 1995, Dallas TX, pp.233-236.","title":"Journal"},{"location":"references/#website","text":"Keyantash, John & National Center for Atmospheric Research Staff (Eds). \"The Climate Data Guide: Standardized Precipitation Index (SPI).\" Retrieved from https://climatedataguide.ucar.edu/climate-data/standardized-precipitation-index-spi National Drought Mitigation Center (NDMC) at the University of Nebraska Lincoln. Link World Meteorological Organization (WMO), 2012: Standardized Precipitation Index User Guide. Link Climate Indices in Python https://climate-indices.readthedocs.io/en/latest/ Climate Engine - https://app.climateengine.org/climateEngine BMKG - https://www.bmkg.go.id/iklim/indeks-presipitasi-terstandarisasi.bmkg NASA ARSET on Application of GPM IMERG Reanalysis for Assessing Extreme Dry and Wet Periods. Link Climate Data Operator - https://code.mpimet.mpg.de/projects/cdo netCDF Operator - http://nco.sourceforge.net CHIRPS, Climate Hazard Centre UCSB - https://chc.ucsb.edu/data/chirps IMERG, Integrated Multi-satellitE Retrievals for GPM - https://gpm.nasa.gov/data/imerg https://benny.istan.to/blog/20200706-calculate-spi-using-imerg-data https://benny.istan.to/blog/20200710-calculate-spi-using-chirps-data https://benny.istan.to/blog/20201210-geotiff-to-netcdf-file-with-time-dimension-enabled-and-cf-compliant https://benny.istan.to/blog/20210125-calculate-spi-using-monthly-rainfall-data-in-geotiff-format","title":"Website"},{"location":"software/","text":"1. Software Requirement If you encounter a problem, please look for a online solution. The installation and configuration described below is mostly performed using a bash shell on macOS. Windows users will need to install and configure a bash shell in order to follow the usage shown below. Try to use Windows Subsystem for Linux for this purpose. macOS/Linux Installing software for macOS/Linux If you are new to using Bash refer to the following lessons with Software Carpentry: http://swcarpentry.github.io/shell-novice/ If you don't have Homebrew , you can install it by pasting below code in your macOS/Linux terminal. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Install wget (for downloading data). Use Hombrew to install it by pasting below code in your macOS terminal. brew install wget Download and install Panoply Data Viewer from NASA GISS on your machine for macOS or Linux . Download and install Anaconda Python on your machine for macOS or Linux . Follow Installing Anaconda on macOS guideline and for Linux Tip Or you can use Miniconda for macOS or Linux . And follow the installation guideline for macOS and Linux Windows Enable the Windows Subsytem for Linux Note If you are using Windows machine, it's recomended to follow below step. You will experience an error during SPI calculation cause by NCO if you use standard Windows 10 and not using Windows Subsytem for Linux. Guideline below are specific for Windows 10. If you are using Windows Server 2019, please follow Windows Server Installation Guide Reference: https://docs.microsoft.com/en-us/windows/wsl/install-win10 You must first enable the \"Windows Subsystem for Linux - WSL\" optional feature before installing any Linux distributions on Windows. Open PowerShell as Administrator (right-click PowerShell) and run: dism . exe / online / enable-feature / featurename : Microsoft-Windows-Subsystem-Linux / all / norestart Update to WSL 2 Check requirements for running WSL 2 To update to WSL 2, you must be running Windows 10. For x64 systems: Version 1903 or higher, with Build 18362 or higher. For ARM64 systems: Version 2004 or higher, with Build 19041 or higher. Builds lower than 18362 do not support WSL 2. Use the Windows Update Assistant to update your version of Windows. To check your version and build number, select Windows logo key + R, type winver, select OK. Update to the latest Windows version in the Settings menu. Note If you are running Windows 10 version 1903 or 1909, open \"Settings\" from your Windows menu, navigate to \"Update & Security\" and select \"Check for Updates\". Your Build number must be 18362.1049+ or 18363.1049+, with the minor build # over .1049. Read more: WSL 2 Support is coming to Windows 10 Versions 1903 and 1909. See the troubleshooting instructions. Enable Virtual Machine feature Before installing WSL 2, you must enable the Virtual Machine Platform optional feature. Your machine will require virtualization capabilities to use this feature. Open PowerShell as Administrator (right-click PowerShell) and run: dism . exe / online / enable-feature / featurename : VirtualMachinePlatform / all / norestart Restart your machine to complete the WSL install and update to WSL 2. Download the Linux kernel update package Download the latest package: WSL2 Linux kernel update package for x64 machines Note If you're using an ARM64 machine, please download the ARM64 package instead. If you're not sure what kind of machine you have, open Command Prompt or PowerShell and enter: systeminfo | find \"System Type\" . Caveat: On non-English Windows versions, you might have to modify the search text, for example, in German it would be systeminfo | find \"Systemtyp\" . Run the update package downloaded in the previous step. (Double-click to run - you will be prompted for elevated permissions, select \u2018yes\u2019 to approve this installation.) Once the installation is complete, move on to the next step - setting WSL 2 as your default version when installing new Linux distributions. (Skip this step if you want your new Linux installs to be set to WSL 1). Note For more information, read the article changes to updating the WSL2 Linux kernel , available on the Windows Command Line Blog . Set WSL 2 as your default version Open PowerShell and run this command to set WSL 2 as the default version when installing a new Linux distribution: wsl - -set-default-version 2 Install your Linux distribution of choice Open the Microsoft Store and select your favourite Linux distribution. Let's focus to use Ubunto 20.04 LTS distro From the distribution's page, select \"Get\" The first time you launch a newly installed Linux distribution, a console window will open and you'll be asked to wait for a minute or two for files to de-compress and be stored on your PC. All future launches should take less than a second. You will then need to create a user account and password for your new Linux distribution . CONGRATULATIONS! You've successfully installed and set up a Linux distribution that is completely integrated with your Windows operating system! Install Windows Terminal (optional) Windows Terminal enables multiple tabs (quickly switch between multiple Linux command lines, Windows Command Prompt, PowerShell, Azure CLI, etc), create custom key bindings (shortcut keys for opening or closing tabs, copy+paste, etc.), use the search feature, and custom themes (color schemes, font styles and sizes, background image/blur/transparency). Learn more . Install Windows Terminal . Set your distribution version to WSL 1 or WSL 2 You can check the WSL version assigned to each of the Linux distributions you have installed by opening the PowerShell command line and entering the command (only available in Windows Build 18362 or higher ): wsl -l -v wsl - -list - -verbose To set a distribution to be backed by either version of WSL please run: wsl - -set-version < distribution name > < versionNumber > Make sure to replace <distribution name> with the actual name of your distribution and <versionNumber> with the number '1' or '2'. You can change back to WSL 1 at anytime by running the same command as above but replacing the '2' with a '1'. Note The update from WSL 1 to WSL 2 may take several minutes to complete depending on the size of your targeted distribution. If you are running an older (legacy) installation of WSL 1 from Windows 10 Anniversary Update or Creators Update, you may encounter an update error. Follow these instructions to uninstall and remove any legacy distributions . If wsl --set-default-version results as an invalid command, enter wsl --help . If the --set-default-version is not listed, it means that your OS doesn't support it and you need to update to version 1903, Build 18362 or higher. If you are on Build 19041 for ARM64, this command may fail when using PowerShell in which case you can use a Command Prompt instead to issue the wsl.exe command. If you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel . You still need to install the MSI Linux kernel update package. Additionally, if you want to make WSL 2 your default architecture you can do so with this command: wsl - -set-default-version 2 This will set the version of any new distribution installed to WSL 2. Installing software for Windows If you have a Bash shell already installed on your Windows OS (e.g. Ubuntu Bash) you can use that for the exercise, but it must be a Bash shell If you are new to using Bash refer to the following lessons with Software Carpentry: http://swcarpentry.github.io/shell-novice/ If you don't have Homebrew , you can install it by pasting below code in your WSL Ubuntu terminal. bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" Install wget (for downloading data). Use Hombrew to install it by pasting below code in your WSL Ubuntu terminal. brew install wget Download and install Panoply Data Viewer from NASA GISS on your machine: Windows . Download and install Anaconda Python on your WSL Ubuntu Linux. : Ubuntu Linux on WSL . Warning climate-indices python package used for SPI calculation is rely on netCDF Operator (NCO) and pyNCO wrapper sometimes produce an error in Windows. That's the reason why we will use Anaconda for Linux if you are using Windows machine. Reference: https://gist.github.com/kauffmanes/5e74916617f9993bc3479f401dfec7da Go to https://repo.anaconda.com/archive/ to find the list of Anaconda releases Select the release you want. I have a 64-bit computer, so I chose the latest release ending in x86_64.sh . If I had a 32-bit computer, I'd select the x86.sh version. If you accidentally try to install the wrong one, you'll get a warning in the terminal. I chose Anaconda3-2020.11-Linux-x86_64.sh . From the terminal run wget https://repo.anaconda.com/archive/[YOUR VERSION] . Example: wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh After download process completed, Run the installation script: bash Anaconda[YOUR VERSION].sh bash Anaconda3-2020.11-Linux-x86_64.sh Read the license agreement and follow the prompts to press Return/Enter to accept. Later will follow with question on accept the license terms, type yes and Enter. When asks you if you'd like the installer to prepend it to the path, press Return/Enter to confirm the location. Last question will be about initialize Anaconda3, type yes then Enter. Close the terminal and reopen it to reload .bash configs. It will automatically activate base environment. Deactivate base environment then set to false the confirguration of auto activate the base environment by typing conda deactivate && conda config --set auto_activate_base false To test that it worked, which python in your Terminal. It should print a path that has anaconda in it. Mine is /home/bennyistanto/anaconda3/bin/python . If it doesn't have anaconda in the path, do the next step. Manually add the Anaconda bin folder to your PATH. To do this, I added \"export PATH=/home/bennyistanto/anaconda3/bin:$PATH\" to the bottom of my ~/.bashrc file. Optionally install Visual Studio Code when prompted Info Or you can use Miniconda : Ubuntu Linux on WSL .","title":"1. Software requirements"},{"location":"software/#1-software-requirement","text":"If you encounter a problem, please look for a online solution. The installation and configuration described below is mostly performed using a bash shell on macOS. Windows users will need to install and configure a bash shell in order to follow the usage shown below. Try to use Windows Subsystem for Linux for this purpose.","title":"1. Software Requirement"},{"location":"software/#macoslinux","text":"","title":"macOS/Linux"},{"location":"software/#installing-software-for-macoslinux","text":"If you are new to using Bash refer to the following lessons with Software Carpentry: http://swcarpentry.github.io/shell-novice/ If you don't have Homebrew , you can install it by pasting below code in your macOS/Linux terminal. /bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh ) \" Install wget (for downloading data). Use Hombrew to install it by pasting below code in your macOS terminal. brew install wget Download and install Panoply Data Viewer from NASA GISS on your machine for macOS or Linux . Download and install Anaconda Python on your machine for macOS or Linux . Follow Installing Anaconda on macOS guideline and for Linux Tip Or you can use Miniconda for macOS or Linux . And follow the installation guideline for macOS and Linux","title":"Installing software for macOS/Linux"},{"location":"software/#windows","text":"","title":"Windows"},{"location":"software/#enable-the-windows-subsytem-for-linux","text":"Note If you are using Windows machine, it's recomended to follow below step. You will experience an error during SPI calculation cause by NCO if you use standard Windows 10 and not using Windows Subsytem for Linux. Guideline below are specific for Windows 10. If you are using Windows Server 2019, please follow Windows Server Installation Guide Reference: https://docs.microsoft.com/en-us/windows/wsl/install-win10 You must first enable the \"Windows Subsystem for Linux - WSL\" optional feature before installing any Linux distributions on Windows. Open PowerShell as Administrator (right-click PowerShell) and run: dism . exe / online / enable-feature / featurename : Microsoft-Windows-Subsystem-Linux / all / norestart","title":"Enable the Windows Subsytem for Linux"},{"location":"software/#update-to-wsl-2","text":"Check requirements for running WSL 2 To update to WSL 2, you must be running Windows 10. For x64 systems: Version 1903 or higher, with Build 18362 or higher. For ARM64 systems: Version 2004 or higher, with Build 19041 or higher. Builds lower than 18362 do not support WSL 2. Use the Windows Update Assistant to update your version of Windows. To check your version and build number, select Windows logo key + R, type winver, select OK. Update to the latest Windows version in the Settings menu. Note If you are running Windows 10 version 1903 or 1909, open \"Settings\" from your Windows menu, navigate to \"Update & Security\" and select \"Check for Updates\". Your Build number must be 18362.1049+ or 18363.1049+, with the minor build # over .1049. Read more: WSL 2 Support is coming to Windows 10 Versions 1903 and 1909. See the troubleshooting instructions.","title":"Update to WSL 2"},{"location":"software/#enable-virtual-machine-feature","text":"Before installing WSL 2, you must enable the Virtual Machine Platform optional feature. Your machine will require virtualization capabilities to use this feature. Open PowerShell as Administrator (right-click PowerShell) and run: dism . exe / online / enable-feature / featurename : VirtualMachinePlatform / all / norestart Restart your machine to complete the WSL install and update to WSL 2.","title":"Enable Virtual Machine feature"},{"location":"software/#download-the-linux-kernel-update-package","text":"Download the latest package: WSL2 Linux kernel update package for x64 machines Note If you're using an ARM64 machine, please download the ARM64 package instead. If you're not sure what kind of machine you have, open Command Prompt or PowerShell and enter: systeminfo | find \"System Type\" . Caveat: On non-English Windows versions, you might have to modify the search text, for example, in German it would be systeminfo | find \"Systemtyp\" . Run the update package downloaded in the previous step. (Double-click to run - you will be prompted for elevated permissions, select \u2018yes\u2019 to approve this installation.) Once the installation is complete, move on to the next step - setting WSL 2 as your default version when installing new Linux distributions. (Skip this step if you want your new Linux installs to be set to WSL 1). Note For more information, read the article changes to updating the WSL2 Linux kernel , available on the Windows Command Line Blog .","title":"Download the Linux kernel update package"},{"location":"software/#set-wsl-2-as-your-default-version","text":"Open PowerShell and run this command to set WSL 2 as the default version when installing a new Linux distribution: wsl - -set-default-version 2","title":"Set WSL 2 as your default version"},{"location":"software/#install-your-linux-distribution-of-choice","text":"Open the Microsoft Store and select your favourite Linux distribution. Let's focus to use Ubunto 20.04 LTS distro From the distribution's page, select \"Get\" The first time you launch a newly installed Linux distribution, a console window will open and you'll be asked to wait for a minute or two for files to de-compress and be stored on your PC. All future launches should take less than a second. You will then need to create a user account and password for your new Linux distribution . CONGRATULATIONS! You've successfully installed and set up a Linux distribution that is completely integrated with your Windows operating system!","title":"Install your Linux distribution of choice"},{"location":"software/#install-windows-terminal-optional","text":"Windows Terminal enables multiple tabs (quickly switch between multiple Linux command lines, Windows Command Prompt, PowerShell, Azure CLI, etc), create custom key bindings (shortcut keys for opening or closing tabs, copy+paste, etc.), use the search feature, and custom themes (color schemes, font styles and sizes, background image/blur/transparency). Learn more . Install Windows Terminal .","title":"Install Windows Terminal (optional)"},{"location":"software/#set-your-distribution-version-to-wsl-1-or-wsl-2","text":"You can check the WSL version assigned to each of the Linux distributions you have installed by opening the PowerShell command line and entering the command (only available in Windows Build 18362 or higher ): wsl -l -v wsl - -list - -verbose To set a distribution to be backed by either version of WSL please run: wsl - -set-version < distribution name > < versionNumber > Make sure to replace <distribution name> with the actual name of your distribution and <versionNumber> with the number '1' or '2'. You can change back to WSL 1 at anytime by running the same command as above but replacing the '2' with a '1'. Note The update from WSL 1 to WSL 2 may take several minutes to complete depending on the size of your targeted distribution. If you are running an older (legacy) installation of WSL 1 from Windows 10 Anniversary Update or Creators Update, you may encounter an update error. Follow these instructions to uninstall and remove any legacy distributions . If wsl --set-default-version results as an invalid command, enter wsl --help . If the --set-default-version is not listed, it means that your OS doesn't support it and you need to update to version 1903, Build 18362 or higher. If you are on Build 19041 for ARM64, this command may fail when using PowerShell in which case you can use a Command Prompt instead to issue the wsl.exe command. If you see this message after running the command: WSL 2 requires an update to its kernel component. For information please visit https://aka.ms/wsl2kernel . You still need to install the MSI Linux kernel update package. Additionally, if you want to make WSL 2 your default architecture you can do so with this command: wsl - -set-default-version 2 This will set the version of any new distribution installed to WSL 2.","title":"Set your distribution version to WSL 1 or WSL 2"},{"location":"software/#installing-software-for-windows","text":"If you have a Bash shell already installed on your Windows OS (e.g. Ubuntu Bash) you can use that for the exercise, but it must be a Bash shell If you are new to using Bash refer to the following lessons with Software Carpentry: http://swcarpentry.github.io/shell-novice/ If you don't have Homebrew , you can install it by pasting below code in your WSL Ubuntu terminal. bin/bash -c \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh ) \" Install wget (for downloading data). Use Hombrew to install it by pasting below code in your WSL Ubuntu terminal. brew install wget Download and install Panoply Data Viewer from NASA GISS on your machine: Windows . Download and install Anaconda Python on your WSL Ubuntu Linux. : Ubuntu Linux on WSL . Warning climate-indices python package used for SPI calculation is rely on netCDF Operator (NCO) and pyNCO wrapper sometimes produce an error in Windows. That's the reason why we will use Anaconda for Linux if you are using Windows machine. Reference: https://gist.github.com/kauffmanes/5e74916617f9993bc3479f401dfec7da Go to https://repo.anaconda.com/archive/ to find the list of Anaconda releases Select the release you want. I have a 64-bit computer, so I chose the latest release ending in x86_64.sh . If I had a 32-bit computer, I'd select the x86.sh version. If you accidentally try to install the wrong one, you'll get a warning in the terminal. I chose Anaconda3-2020.11-Linux-x86_64.sh . From the terminal run wget https://repo.anaconda.com/archive/[YOUR VERSION] . Example: wget https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh After download process completed, Run the installation script: bash Anaconda[YOUR VERSION].sh bash Anaconda3-2020.11-Linux-x86_64.sh Read the license agreement and follow the prompts to press Return/Enter to accept. Later will follow with question on accept the license terms, type yes and Enter. When asks you if you'd like the installer to prepend it to the path, press Return/Enter to confirm the location. Last question will be about initialize Anaconda3, type yes then Enter. Close the terminal and reopen it to reload .bash configs. It will automatically activate base environment. Deactivate base environment then set to false the confirguration of auto activate the base environment by typing conda deactivate && conda config --set auto_activate_base false To test that it worked, which python in your Terminal. It should print a path that has anaconda in it. Mine is /home/bennyistanto/anaconda3/bin/python . If it doesn't have anaconda in the path, do the next step. Manually add the Anaconda bin folder to your PATH. To do this, I added \"export PATH=/home/bennyistanto/anaconda3/bin:$PATH\" to the bottom of my ~/.bashrc file. Optionally install Visual Studio Code when prompted Info Or you can use Miniconda : Ubuntu Linux on WSL .","title":"Installing software for Windows"},{"location":"spi/","text":"4. Calculate SPI Let's start the calculation! Deactivate an active environment gis then activate environment climate_indices to start working on SPI calculation. conda deactivate && conda activate climate_indices Note Please make sure the input file are following input requirements for SPI Variable name on precipitation --var_name_precip , usually IMERG data use precipitation as name while CHIRPS using precip . To make sure, check using command ncdump -h file.nc then adjust it in SPI script if needed. Precipitation unit must be written as millimeters , milimeter , mm , inches , inch or in . Data dimension and order must be written as lat , lon , time (Windows machine required this order) or time , lat , lon (Works tested on Mac/Linux and Linux running on WSL). IMERG data Navigate your Terminal to folder Input_nc In order to pre-compute fitting parameters for later use as inputs to subsequent SPI calculations, I can save both gamma and Pearson distribution fitting parameters to NetCDF, and later use this file as input for SPI calculations over the same calibration period. Make sure you check below information: Input file java_cli_imerg_1months_2000_2020.nc available at Input_nc folder. Example: /Users/bennyistanto/Temp/IMERG/Java/Input_nc/ Output folder named Output_nc to save SPI result is exist in your working directory Output folder named Fitting to store fitting result is exist in your working directory Variable name is precipitation In your Terminal, run the following code. spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 2000 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/IMERG/SPI/Java/Input_nc/java_cli_imerg_1months_2000_2020.nc --var_name_precip precipitation --output_file_base /Users/bennyistanto/Temp/IMERG/SPI/Java/Output_nc/java_IMERG --multiprocessing all --save_params /Users/bennyistanto/Temp/IMERG/SPI/Java/Fitting/java_IMERG_fitting.nc --overwrite Tip Above code is example for calculating SPI 1 to 72-months. It's ok if you think you only need some of them. Example: you are interested to calculate SPI 1 - 3-months, then adjust above code into --scales 1 2 3 The above command will compute SPI (standardized precipitation index, both gamma and Pearson Type III distributions) from an input precipitation dataset (in this case, IMERG precipitation dataset). The input dataset is monthly rainfall accumulation data and the calibration period used will be Jun-2000 through Dec-2020. The index will be computed at 1 , 2 , 3 , 6 , 9 , 12 , 24 , 36 , 48 , 60 and 72-month timescales. The output files will be < out_dir>/java_IMERG_spi_gamma_xx.nc , and <out_dir>/java_IMERG_spi_pearson_xx.nc . The output files will be: Gamma 1-month: /Output_nc/java_IMERG_spi_gamma_01.nc 2-month: /Output_nc/java_IMERG_spi_gamma_02.nc 3-month: /Output_nc/java_IMERG_spi_gamma_03.nc 6-month: /Output_nc/java_IMERG_spi_gamma_06.nc 9-month: /Output_nc/java_IMERG_spi_gamma_09.nc 12-month: /Output_nc/java_IMERG_spi_gamma_12.nc 24-month: /Output_nc/java_IMERG_spi_gamma_24.nc 36-month: /Output_nc/java_IMERG_spi_gamma_36.nc 48-month: /Output_nc/java_IMERG_spi_gamma_48.nc 60-month: /Output_nc/java_IMERG_spi_gamma_60.nc 72-month: /Output_nc/java_IMERG_spi_gamma_72.nc Pearson 1-month: /Output_nc/java_IMERG_spi_pearson_01.nc 2-month: /Output_nc/java_IMERG_spi_pearson_02.nc 3-month: /Output_nc/java_IMERG_spi_pearson_03.nc 6-month: /Output_nc/java_IMERG_spi_pearson_06.nc 9-month: /Output_nc/java_IMERG_spi_pearson_09.nc 12-month: /Output_nc/java_IMERG_spi_pearson_12.nc 24-month: /Output_nc/java_IMERG_spi_pearson_24.nc 36-month: /Output_nc/java_IMERG_spi_pearson_36.nc 48-month: /Output_nc/java_IMERG_spi_pearson_48.nc 60-month: /Output_nc/java_IMERG_spi_pearson_60.nc 72-month: /Output_nc/java_IMERG_spi_pearson_72.nc Parallelization will occur utilizing all CPUs. CHIRPS data To make sure everything is correct and following above specifications, in your Terminal - navigate to your directory where you save java_cli_chirps_1months_1981_2020.nc file: Input_nc . Then type ncdump -h java_cli_chirps_1months_1981_2020.nc From above picture, I can say: Time dimension is enabled, 480 is total months from Jan 1981 to Dec 2020 Data dimension and order are following the specification time , lat , lon The unit is in mm So, everything is correct and I am ready to calculate SPI. Make sure in your Terminal still inside climate_indices environment. Other requirements and options related to the indices calculation, please follow https://climate-indices.readthedocs.io/en/latest/#indices-processing In order to pre-compute fitting parameters for later use as inputs to subsequent SPI calculations I can save both gamma and Pearson distribution fitting parameters to NetCDF, and later use this file as input for SPI calculations over the same calibration period. Make sure you check below information: Input file java_cli_chirps_1months_1981_2021.nc available at Input_nc folder. Example: /Users/bennyistanto/Temp/CHIRPS/Java/Input_nc/ Output folder named Output_nc to save SPI result is exist in your working directory Output folder named Fitting to store fitting result is exist in your working directory Variable name is precip In your Terminal, run the following code. spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 1981 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/CHIRPS/SPI/Java/Input_nc/java_cli_chirps_1months_1981_2021.nc --var_name_precip precip --output_file_base /Users/bennyistanto/Temp/CHIRPS/SPI/Java/Output_nc/java_CHIRPS --multiprocessing all --save_params /Users/bennyistanto/Temp/CHIRPS/SPI/Java/Fitting/java_CHIRPS_fitting.nc --overwrite Tip Above code is example for calculating SPI 1 to 72-months. It's ok if you think you only need some of them. Example: you are interested to calculate SPI 1 - 3-months, then adjust above code into --scales 1 2 3 The above command will compute SPI (standardized precipitation index, both gamma and Pearson Type III distributions) from an input precipitation dataset (in this case, CHIRPS precipitation dataset). The input dataset is monthly rainfall accumulation data and the calibration period used will be Jan-1981 through Dec-2020. The index will be computed at 1 , 2 , 3 , 6 , 9 , 12 , 24 , 36 , 48 , 60 and 72-month timescales. The output files will be < out_dir>/java_CHIRPS_spi_gamma_xx.nc , and <out_dir>/java_CHIRPS_spi_pearson_xx.nc . The output files will be: Gamma 1-month: /Output_nc/java_CHIRPS_spi_gamma_01.nc 2-month: /Output_nc/java_CHIRPS_spi_gamma_02.nc 3-month: /Output_nc/java_CHIRPS_spi_gamma_03.nc 6-month: /Output_nc/java_CHIRPS_spi_gamma_06.nc 9-month: /Output_nc/java_CHIRPS_spi_gamma_09.nc 12-month: /Output_nc/java_CHIRPS_spi_gamma_12.nc 24-month: /Output_nc/java_CHIRPS_spi_gamma_24.nc 36-month: /Output_nc/java_CHIRPS_spi_gamma_36.nc 48-month: /Output_nc/java_CHIRPS_spi_gamma_48.nc 60-month: /Output_nc/java_CHIRPS_spi_gamma_60.nc 72-month: /Output_nc/java_CHIRPS_spi_gamma_72.nc Pearson 1-month: /Output_nc/java_CHIRPS_spi_pearson_01.nc 2-month: /Output_nc/java_CHIRPS_spi_pearson_02.nc 3-month: /Output_nc/java_CHIRPS_spi_pearson_03.nc 6-month: /Output_nc/java_CHIRPS_spi_pearson_06.nc 9-month: /Output_nc/java_CHIRPS_spi_pearson_09.nc 12-month: /Output_nc/java_CHIRPS_spi_pearson_12.nc 24-month: /Output_nc/java_CHIRPS_spi_pearson_24.nc 36-month: /Output_nc/java_CHIRPS_spi_pearson_36.nc 48-month: /Output_nc/java_CHIRPS_spi_pearson_48.nc 60-month: /Output_nc/java_CHIRPS_spi_pearson_60.nc 72-month: /Output_nc/java_CHIRPS_spi_pearson_72.nc Parallelization will occur utilizing all CPUs. Time processing For small area of interest, the calculation will fast and don\u2019t take much time. Below is one of example if you processed bigger area: Monthly IMERG data, global coverage 180W - 180E, 60N - 60S, 0.1 deg spatial resolution. It takes almost 9-hours to process SPI 1-72 months. Output gamma and pearson file https://github.com/wfpidn/SPI/blob/master/Data/Output_nc Fitting file https://github.com/wfpidn/SPI/blob/master/Data/Fitting","title":"4. Calculate SPI"},{"location":"spi/#4-calculate-spi","text":"Let's start the calculation! Deactivate an active environment gis then activate environment climate_indices to start working on SPI calculation. conda deactivate && conda activate climate_indices Note Please make sure the input file are following input requirements for SPI Variable name on precipitation --var_name_precip , usually IMERG data use precipitation as name while CHIRPS using precip . To make sure, check using command ncdump -h file.nc then adjust it in SPI script if needed. Precipitation unit must be written as millimeters , milimeter , mm , inches , inch or in . Data dimension and order must be written as lat , lon , time (Windows machine required this order) or time , lat , lon (Works tested on Mac/Linux and Linux running on WSL).","title":"4. Calculate SPI"},{"location":"spi/#imerg-data","text":"Navigate your Terminal to folder Input_nc In order to pre-compute fitting parameters for later use as inputs to subsequent SPI calculations, I can save both gamma and Pearson distribution fitting parameters to NetCDF, and later use this file as input for SPI calculations over the same calibration period. Make sure you check below information: Input file java_cli_imerg_1months_2000_2020.nc available at Input_nc folder. Example: /Users/bennyistanto/Temp/IMERG/Java/Input_nc/ Output folder named Output_nc to save SPI result is exist in your working directory Output folder named Fitting to store fitting result is exist in your working directory Variable name is precipitation In your Terminal, run the following code. spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 2000 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/IMERG/SPI/Java/Input_nc/java_cli_imerg_1months_2000_2020.nc --var_name_precip precipitation --output_file_base /Users/bennyistanto/Temp/IMERG/SPI/Java/Output_nc/java_IMERG --multiprocessing all --save_params /Users/bennyistanto/Temp/IMERG/SPI/Java/Fitting/java_IMERG_fitting.nc --overwrite Tip Above code is example for calculating SPI 1 to 72-months. It's ok if you think you only need some of them. Example: you are interested to calculate SPI 1 - 3-months, then adjust above code into --scales 1 2 3 The above command will compute SPI (standardized precipitation index, both gamma and Pearson Type III distributions) from an input precipitation dataset (in this case, IMERG precipitation dataset). The input dataset is monthly rainfall accumulation data and the calibration period used will be Jun-2000 through Dec-2020. The index will be computed at 1 , 2 , 3 , 6 , 9 , 12 , 24 , 36 , 48 , 60 and 72-month timescales. The output files will be < out_dir>/java_IMERG_spi_gamma_xx.nc , and <out_dir>/java_IMERG_spi_pearson_xx.nc . The output files will be: Gamma 1-month: /Output_nc/java_IMERG_spi_gamma_01.nc 2-month: /Output_nc/java_IMERG_spi_gamma_02.nc 3-month: /Output_nc/java_IMERG_spi_gamma_03.nc 6-month: /Output_nc/java_IMERG_spi_gamma_06.nc 9-month: /Output_nc/java_IMERG_spi_gamma_09.nc 12-month: /Output_nc/java_IMERG_spi_gamma_12.nc 24-month: /Output_nc/java_IMERG_spi_gamma_24.nc 36-month: /Output_nc/java_IMERG_spi_gamma_36.nc 48-month: /Output_nc/java_IMERG_spi_gamma_48.nc 60-month: /Output_nc/java_IMERG_spi_gamma_60.nc 72-month: /Output_nc/java_IMERG_spi_gamma_72.nc Pearson 1-month: /Output_nc/java_IMERG_spi_pearson_01.nc 2-month: /Output_nc/java_IMERG_spi_pearson_02.nc 3-month: /Output_nc/java_IMERG_spi_pearson_03.nc 6-month: /Output_nc/java_IMERG_spi_pearson_06.nc 9-month: /Output_nc/java_IMERG_spi_pearson_09.nc 12-month: /Output_nc/java_IMERG_spi_pearson_12.nc 24-month: /Output_nc/java_IMERG_spi_pearson_24.nc 36-month: /Output_nc/java_IMERG_spi_pearson_36.nc 48-month: /Output_nc/java_IMERG_spi_pearson_48.nc 60-month: /Output_nc/java_IMERG_spi_pearson_60.nc 72-month: /Output_nc/java_IMERG_spi_pearson_72.nc Parallelization will occur utilizing all CPUs.","title":"IMERG data"},{"location":"spi/#chirps-data","text":"To make sure everything is correct and following above specifications, in your Terminal - navigate to your directory where you save java_cli_chirps_1months_1981_2020.nc file: Input_nc . Then type ncdump -h java_cli_chirps_1months_1981_2020.nc From above picture, I can say: Time dimension is enabled, 480 is total months from Jan 1981 to Dec 2020 Data dimension and order are following the specification time , lat , lon The unit is in mm So, everything is correct and I am ready to calculate SPI. Make sure in your Terminal still inside climate_indices environment. Other requirements and options related to the indices calculation, please follow https://climate-indices.readthedocs.io/en/latest/#indices-processing In order to pre-compute fitting parameters for later use as inputs to subsequent SPI calculations I can save both gamma and Pearson distribution fitting parameters to NetCDF, and later use this file as input for SPI calculations over the same calibration period. Make sure you check below information: Input file java_cli_chirps_1months_1981_2021.nc available at Input_nc folder. Example: /Users/bennyistanto/Temp/CHIRPS/Java/Input_nc/ Output folder named Output_nc to save SPI result is exist in your working directory Output folder named Fitting to store fitting result is exist in your working directory Variable name is precip In your Terminal, run the following code. spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 1981 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/CHIRPS/SPI/Java/Input_nc/java_cli_chirps_1months_1981_2021.nc --var_name_precip precip --output_file_base /Users/bennyistanto/Temp/CHIRPS/SPI/Java/Output_nc/java_CHIRPS --multiprocessing all --save_params /Users/bennyistanto/Temp/CHIRPS/SPI/Java/Fitting/java_CHIRPS_fitting.nc --overwrite Tip Above code is example for calculating SPI 1 to 72-months. It's ok if you think you only need some of them. Example: you are interested to calculate SPI 1 - 3-months, then adjust above code into --scales 1 2 3 The above command will compute SPI (standardized precipitation index, both gamma and Pearson Type III distributions) from an input precipitation dataset (in this case, CHIRPS precipitation dataset). The input dataset is monthly rainfall accumulation data and the calibration period used will be Jan-1981 through Dec-2020. The index will be computed at 1 , 2 , 3 , 6 , 9 , 12 , 24 , 36 , 48 , 60 and 72-month timescales. The output files will be < out_dir>/java_CHIRPS_spi_gamma_xx.nc , and <out_dir>/java_CHIRPS_spi_pearson_xx.nc . The output files will be: Gamma 1-month: /Output_nc/java_CHIRPS_spi_gamma_01.nc 2-month: /Output_nc/java_CHIRPS_spi_gamma_02.nc 3-month: /Output_nc/java_CHIRPS_spi_gamma_03.nc 6-month: /Output_nc/java_CHIRPS_spi_gamma_06.nc 9-month: /Output_nc/java_CHIRPS_spi_gamma_09.nc 12-month: /Output_nc/java_CHIRPS_spi_gamma_12.nc 24-month: /Output_nc/java_CHIRPS_spi_gamma_24.nc 36-month: /Output_nc/java_CHIRPS_spi_gamma_36.nc 48-month: /Output_nc/java_CHIRPS_spi_gamma_48.nc 60-month: /Output_nc/java_CHIRPS_spi_gamma_60.nc 72-month: /Output_nc/java_CHIRPS_spi_gamma_72.nc Pearson 1-month: /Output_nc/java_CHIRPS_spi_pearson_01.nc 2-month: /Output_nc/java_CHIRPS_spi_pearson_02.nc 3-month: /Output_nc/java_CHIRPS_spi_pearson_03.nc 6-month: /Output_nc/java_CHIRPS_spi_pearson_06.nc 9-month: /Output_nc/java_CHIRPS_spi_pearson_09.nc 12-month: /Output_nc/java_CHIRPS_spi_pearson_12.nc 24-month: /Output_nc/java_CHIRPS_spi_pearson_24.nc 36-month: /Output_nc/java_CHIRPS_spi_pearson_36.nc 48-month: /Output_nc/java_CHIRPS_spi_pearson_48.nc 60-month: /Output_nc/java_CHIRPS_spi_pearson_60.nc 72-month: /Output_nc/java_CHIRPS_spi_pearson_72.nc Parallelization will occur utilizing all CPUs.","title":"CHIRPS data"},{"location":"spi/#time-processing","text":"For small area of interest, the calculation will fast and don\u2019t take much time. Below is one of example if you processed bigger area: Monthly IMERG data, global coverage 180W - 180E, 60N - 60S, 0.1 deg spatial resolution. It takes almost 9-hours to process SPI 1-72 months. Output gamma and pearson file https://github.com/wfpidn/SPI/blob/master/Data/Output_nc Fitting file https://github.com/wfpidn/SPI/blob/master/Data/Fitting","title":"Time processing"},{"location":"update/","text":"5. Updating procedure when new data is available What if the new data is coming (Jan 2021)? Should I re-run again for the whole periods, 1981 to date? That's not practical as it requires large storage and time processing if you do for bigger coverage (country or regional analysis). So far, updating the SPI process is easy if I used CHIRPS in GeoTIFF format. Below are some reason: Downloading new CHIRPS data in netCDF is painful, because I need to download whole package data (6.4 GB and start from Jan 1981 to date) eventhough I only need the latest month. CHIRPS data in GeoTIFF provides 1 month 1 GeoTIFF file, only take what you need! IMERG included data over the sea, and easiest way to clipped netCDF data is using bounding box. This approach will not have a problem if all of our area interest is in land. Updating SPI up to SPI-72, I should have data at least 6 years back (2014) from the latest (Jan 2021). In order to avoid computation for the whole periods (1981-2021), I will process data data only for year 2014 to 2021. After that, I continue the process following Step 3 to do conversion process to netCDF format from bunch of GeoTIFF file in a folder with time dimension enabled. Step 4 demonstrates how distribution fitting parameters can be saved as NetCDF. This fittings NetCDF can then be used as pre-computed variables in subsequent SPI computations. Initial command computes both distribution fitting values and SPI for various month scales. The distribution fitting variables are written to the file specified by the --save_params option. The below command also computes SPI but instead of computing the distribution fitting values it loads the pre-computed fitting values from the NetCDF file specified by the --load_params option. See below code: spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 1981 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/CHIRPS/Java/Input_nc/java_cli_chirps_1months_2014_2021.nc --var_name_precip precip --output_file_base /Users/bennyistanto/Temp/CHIRPS/Java/Output_nc/java_CHIRPS --multiprocessing all --load_params /Users/bennyistanto/Temp/CHIRPS/Java/Fitting/java_CHIRPS_fitting.nc","title":"5. Update procedure"},{"location":"update/#5-updating-procedure-when-new-data-is-available","text":"What if the new data is coming (Jan 2021)? Should I re-run again for the whole periods, 1981 to date? That's not practical as it requires large storage and time processing if you do for bigger coverage (country or regional analysis). So far, updating the SPI process is easy if I used CHIRPS in GeoTIFF format. Below are some reason: Downloading new CHIRPS data in netCDF is painful, because I need to download whole package data (6.4 GB and start from Jan 1981 to date) eventhough I only need the latest month. CHIRPS data in GeoTIFF provides 1 month 1 GeoTIFF file, only take what you need! IMERG included data over the sea, and easiest way to clipped netCDF data is using bounding box. This approach will not have a problem if all of our area interest is in land. Updating SPI up to SPI-72, I should have data at least 6 years back (2014) from the latest (Jan 2021). In order to avoid computation for the whole periods (1981-2021), I will process data data only for year 2014 to 2021. After that, I continue the process following Step 3 to do conversion process to netCDF format from bunch of GeoTIFF file in a folder with time dimension enabled. Step 4 demonstrates how distribution fitting parameters can be saved as NetCDF. This fittings NetCDF can then be used as pre-computed variables in subsequent SPI computations. Initial command computes both distribution fitting values and SPI for various month scales. The distribution fitting variables are written to the file specified by the --save_params option. The below command also computes SPI but instead of computing the distribution fitting values it loads the pre-computed fitting values from the NetCDF file specified by the --load_params option. See below code: spi --periodicity monthly --scales 1 2 3 6 9 12 24 36 48 60 72 --calibration_start_year 1981 --calibration_end_year 2020 --netcdf_precip /Users/bennyistanto/Temp/CHIRPS/Java/Input_nc/java_cli_chirps_1months_2014_2021.nc --var_name_precip precip --output_file_base /Users/bennyistanto/Temp/CHIRPS/Java/Output_nc/java_CHIRPS --multiprocessing all --load_params /Users/bennyistanto/Temp/CHIRPS/Java/Fitting/java_CHIRPS_fitting.nc","title":"5. Updating procedure when new data is available"},{"location":"vizualisation/","text":"6. Visualize the result using Panoply Let see the result. From the Output_nc directory, right-click file java_CHIRPS_spi_gamma_3_month.nc and Open With Panoply. If you are not following the tutorial but interested to see the file, you can download this file from this link: https://on.istan.to/2MhVnTP From the Datasets tab select spi_gamma_3_month and click Create Plot In the Create Plot window select option Georeferenced Longitude-Latitude. When the Plot window opens: Array tab: Change the time into 469 to view data on Jan 2020 Scale tab: Change value on Min -3 , Max 3 , Major 6 , Color Table CB_RdBu_09.cpt Map tab: Change value on Center on Lon 110.0 Lat -7.5 , then Zoom in the map through menu-editor Plot > Zoom - Plot In few times until Indonesia appear proportionally. Set grid spacing 2.0 and Labels on every grid lines. Overlays tab: Change Overlay 1 to MWDB_Coasts_Countries_1.cnob","title":"6. Visualize the result using Panoply"},{"location":"vizualisation/#6-visualize-the-result-using-panoply","text":"Let see the result. From the Output_nc directory, right-click file java_CHIRPS_spi_gamma_3_month.nc and Open With Panoply. If you are not following the tutorial but interested to see the file, you can download this file from this link: https://on.istan.to/2MhVnTP From the Datasets tab select spi_gamma_3_month and click Create Plot In the Create Plot window select option Georeferenced Longitude-Latitude. When the Plot window opens: Array tab: Change the time into 469 to view data on Jan 2020 Scale tab: Change value on Min -3 , Max 3 , Major 6 , Color Table CB_RdBu_09.cpt Map tab: Change value on Center on Lon 110.0 Lat -7.5 , then Zoom in the map through menu-editor Plot > Zoom - Plot In few times until Indonesia appear proportionally. Set grid spacing 2.0 and Labels on every grid lines. Overlays tab: Change Overlay 1 to MWDB_Coasts_Countries_1.cnob","title":"6. Visualize the result using Panoply"}]}